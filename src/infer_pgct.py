"""
PGCT æ¨¡å‹æ¨ç†è„šæœ¬ï¼ˆå•æ¡æˆ–æ‰¹é‡æ–‡æœ¬æ–‡ä»¶ï¼‰
é€‚é… Transformer + Pointer-Generator + Coverage æ¨¡å‹
æ”¯æŒ OOV è¯å¤„ç†ä¸ Greedy/Beam è§£ç 
"""
import os
import sys
import time
from pathlib import Path
import argparse
from typing import List, Dict

import torch
import yaml
import nltk
from nltk.tokenize import word_tokenize

# ç¡®ä¿ NLTK åˆ†è¯èµ„æºå­˜åœ¨
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

# æ·»åŠ é¡¹ç›®æ ¹è·¯å¾„åˆ°ç¯å¢ƒå˜é‡ï¼Œç¡®ä¿èƒ½å¯¼å…¥å†…éƒ¨æ¨¡å—
sys.path.insert(0, str(Path(__file__).parent))

# å…³é”®ä¿®æ”¹1ï¼šå¯¼å…¥ PGCT ä¸“ç”¨è§£ç å‡½æ•°ï¼ˆæ›¿æ¢é€šç”¨è§£ç ï¼‰
from models.pgct.pgct_model import PGCTModel
from models.pgct.pgct_decoding import pgct_greedy_decode, pgct_beam_search_decode
from utils.vocab import Vocab


def tokenize(text: str, lowercase: bool = True) -> List[str]:
    """æ–‡æœ¬åˆ†è¯ï¼ˆä¸æ•°æ®é¢„å¤„ç†é€»è¾‘ä¸€è‡´ï¼‰"""
    if lowercase:
        text = text.lower()
    return word_tokenize(text)


def process_oov(tokens: List[str], vocab: Vocab) -> tuple[List[int], Dict[int, str], List[int]]:
    """
    å¤„ç† OOV è¯ï¼Œç”Ÿæˆï¼š
    - src_indicesï¼šæºæ–‡æœ¬åœ¨åŸºç¡€è¯è¡¨ä¸­çš„ç´¢å¼•ï¼ˆOOV ç”¨ UNK ç´¢å¼•ï¼‰
    - oov_dictï¼šOOV è¯æ˜ å°„ï¼ˆkeyï¼šç›¸å¯¹ç´¢å¼•ï¼Œvalueï¼šOOV è¯ï¼‰
    - src_oov_mapï¼šOOV è¯çš„ç›¸å¯¹ç´¢å¼•ï¼ˆä¸ src_indices é•¿åº¦ä¸€è‡´ï¼Œé OOV å¡« -1ï¼‰
    """
    src_indices = []  # åŸºç¡€è¯è¡¨ç´¢å¼•ï¼ˆå« UNKï¼‰
    oov_dict = {}     # å­˜å‚¨ OOV è¯ï¼š{ç›¸å¯¹ç´¢å¼•: OOVè¯}ï¼ˆç›¸å¯¹ç´¢å¼•ä» 0 å¼€å§‹ï¼‰
    src_oov_map = []  # å¯¹åº” src_indicesï¼ŒOOV è¯å¡«ç›¸å¯¹ç´¢å¼•ï¼Œæ™®é€šè¯å¡« -1

    for token in tokens:
        if token in vocab.word2idx:
            # æ™®é€šè¯ï¼šç”¨åŸºç¡€è¯è¡¨ç´¢å¼•ï¼ŒOOV æ˜ å°„å¡« -1
            src_indices.append(vocab.word2idx[token])
            src_oov_map.append(-1)
        else:
            # OOV è¯ï¼šåˆ†é…ç›¸å¯¹ç´¢å¼•ï¼ŒåŸºç¡€è¯è¡¨ç´¢å¼•ç”¨ UNK
            if token not in oov_dict.values():
                new_oov_idx = len(oov_dict)  # ç›¸å¯¹ç´¢å¼•ä» 0 å¼€å§‹
                oov_dict[new_oov_idx] = token
            oov_idx = [k for k, v in oov_dict.items() if v == token][0]
            src_indices.append(vocab.unk_idx)  # åŸºç¡€è¯è¡¨ç”¨ UNK
            src_oov_map.append(oov_idx)        # OOV æ˜ å°„å¡«ç›¸å¯¹ç´¢å¼•

    return src_indices, oov_dict, src_oov_map


def load_pgct_model(
    checkpoint_path: str, 
    vocab_size: int, 
    pad_idx: int, 
    device: torch.device,
    config: Dict = None
) -> tuple[PGCTModel, Dict]:
    """åŠ è½½ PGCT æ¨¡å‹ï¼ˆå…¼å®¹å¸¦ config å’Œçº¯å‚æ•°çš„ checkpointï¼‰"""
    checkpoint = torch.load(checkpoint_path, map_location=device)
    # ä¼˜å…ˆä»å¤–éƒ¨ config è·å–å‚æ•°ï¼Œå…¶æ¬¡ä» checkpoint å†…çš„ configï¼Œæœ€åç”¨é»˜è®¤å€¼
    cfg = config.get('model', {}) if config else {}
    checkpoint_cfg = checkpoint.get('config', {})

    # æ¨¡å‹æ ¸å¿ƒå‚æ•°ï¼ˆä¸ PGCTModel åˆå§‹åŒ–å‚æ•°ä¸¥æ ¼å¯¹é½ï¼‰
    model_cfg = {
        'embed_size': cfg.get('embed_size', checkpoint_cfg.get('embed_size', 256)),
        'hidden_size': cfg.get('hidden_size', checkpoint_cfg.get('hidden_size', 256)),
        'num_encoder_layers': cfg.get('num_encoder_layers', checkpoint_cfg.get('num_encoder_layers', 3)),
        'num_decoder_layers': cfg.get('num_decoder_layers', checkpoint_cfg.get('num_decoder_layers', 3)),
        'nhead': cfg.get('nhead', checkpoint_cfg.get('nhead', 8)),
        'dropout': cfg.get('dropout', checkpoint_cfg.get('dropout', 0.1)),
        'cov_loss_weight': cfg.get('cov_loss_weight', checkpoint_cfg.get('cov_loss_weight', 1.0)),
        'max_src_len': cfg.get('max_src_len', checkpoint_cfg.get('max_src_len', 400)),
        'max_tgt_len': cfg.get('max_tgt_len', checkpoint_cfg.get('max_tgt_len', 100))
    }

    # åˆå§‹åŒ– PGCT æ¨¡å‹
    model = PGCTModel(
        vocab_size=vocab_size,
        embed_size=model_cfg['embed_size'],
        hidden_size=model_cfg['hidden_size'],
        num_encoder_layers=model_cfg['num_encoder_layers'],
        num_decoder_layers=model_cfg['num_decoder_layers'],
        nhead=model_cfg['nhead'],
        dropout=model_cfg['dropout'],
        pad_idx=pad_idx,
        cov_loss_weight=model_cfg['cov_loss_weight'],
        max_src_len=model_cfg['max_src_len'],
        max_tgt_len=model_cfg['max_tgt_len']
    ).to(device)

    # åŠ è½½æ¨¡å‹å‚æ•°ï¼ˆå…¼å®¹ä¸¤ç§ checkpoint æ ¼å¼ï¼‰
    if 'model_state_dict' in checkpoint:
        model.load_state_dict(checkpoint['model_state_dict'])
    else:
        model.load_state_dict(checkpoint)  # çº¯å‚æ•°æ–‡ä»¶ï¼ˆå¦‚ best_model.ptï¼‰

    model.eval()  # æ¨ç†æ¨¡å¼ï¼šå…³é—­ Dropout å’Œ BatchNorm
    return model, model_cfg


def summarize_single_text(
    model: PGCTModel,
    vocab: Vocab,
    article: str,
    device: torch.device,
    max_src_len: int = 400,
    max_tgt_len: int = 100,
    decode_strategy: str = 'greedy',
    beam_size: int = 5
) -> str:
    """å•æ¡æ–‡æœ¬ç”Ÿæˆæ‘˜è¦ï¼ˆæ ¸å¿ƒæ¨ç†é€»è¾‘ï¼‰"""
    # 1. æ–‡æœ¬é¢„å¤„ç†ï¼šåˆ†è¯ + æˆªæ–­ï¼ˆä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
    tokens = tokenize(article)[:max_src_len]  # æˆªæ–­åˆ°æœ€å¤§æºæ–‡æœ¬é•¿åº¦
    src_len = len(tokens)  # å®é™…æœ‰æ•ˆé•¿åº¦ï¼ˆä¸å« paddingï¼‰

    # 2. OOV å¤„ç†ï¼šç”ŸæˆåŸºç¡€ç´¢å¼•ã€OOV æ˜ å°„ï¼ˆé€‚é… Pointer-Generatorï¼‰
    src_indices, oov_dict, src_oov_map = process_oov(tokens, vocab)

    # 3. Paddingï¼šè¡¥åˆ° max_src_len é•¿åº¦ï¼ˆä¿è¯è¾“å…¥ç»´åº¦ä¸€è‡´ï¼‰
    if len(src_indices) < max_src_len:
        pad_len = max_src_len - len(src_indices)
        src_indices += [vocab.pad_idx] * pad_len  # åŸºç¡€ç´¢å¼•è¡¥ PAD
        src_oov_map += [-1] * pad_len             # OOV æ˜ å°„è¡¥ -1ï¼ˆæ ‡è®°é OOVï¼‰

    # 4. è½¬æ¢ä¸ºå¼ é‡ï¼ˆbatch_size=1ï¼Œé€‚é…æ¨¡å‹è¾“å…¥æ ¼å¼ï¼‰
    src_tensor = torch.LongTensor([src_indices]).to(device)  # [1, max_src_len]
    src_len_tensor = torch.LongTensor([src_len]).to(device)  # [1]ï¼ˆæœ‰æ•ˆé•¿åº¦ï¼Œç”¨äºæ©ç ï¼‰
    src_oov_tensor = torch.LongTensor([src_oov_map]).to(device)  # [1, max_src_len]

    # 5. æ¨¡å‹æ¨ç†ï¼šè°ƒç”¨ PGCT ä¸“ç”¨è§£ç å‡½æ•°
    with torch.no_grad():  # å…³é—­æ¢¯åº¦è®¡ç®—ï¼ŒèŠ‚çœæ˜¾å­˜å¹¶åŠ é€Ÿ
        if decode_strategy == 'beam':
            pred_ids, _ = pgct_beam_search_decode(
                model=model,
                src=src_tensor,
                src_lens=src_len_tensor,
                src_oov_map=src_oov_tensor,
                max_length=max_tgt_len,
                sos_idx=vocab.sos_idx,
                eos_idx=vocab.eos_idx,
                beam_size=beam_size,
                device=device
            )
        else:
            pred_ids, _ = pgct_greedy_decode(
                model=model,
                src=src_tensor,
                src_lens=src_len_tensor,
                src_oov_map=src_oov_tensor,
                max_length=max_tgt_len,
                sos_idx=vocab.sos_idx,
                eos_idx=vocab.eos_idx,
                device=device
            )

    # 6. ç»“æœè½¬æ¢ï¼šç´¢å¼• â†’ æ–‡æœ¬ï¼ˆå¤„ç† OOV è¯ï¼Œè¿˜åŸåŸå§‹è¯æ±‡ï¼‰
    summary_tokens = []
    for idx in pred_ids.squeeze().tolist():  # å‹ç¼© batch ç»´åº¦ï¼Œè½¬ä¸ºåˆ—è¡¨
        idx_val = idx if isinstance(idx, int) else idx.item()
        if idx_val < len(vocab):
            # æ™®é€šè¯ï¼šä»åŸºç¡€è¯è¡¨è·å–
            token = vocab.idx2word.get(idx_val, vocab.unk_token)
        else:
            # OOV è¯ï¼šè®¡ç®—ç›¸å¯¹ç´¢å¼•ï¼ˆidx_val = åŸºç¡€è¯è¡¨å¤§å° + ç›¸å¯¹ç´¢å¼•ï¼‰
            oov_rel_idx = idx_val - len(vocab)
            token = oov_dict.get(oov_rel_idx, vocab.unk_token)
        # è·³è¿‡ç‰¹æ®Šç¬¦å·ï¼ˆPAD/SOS/EOSï¼Œä¸åŠ å…¥æœ€ç»ˆæ‘˜è¦ï¼‰
        if token not in [vocab.pad_token, vocab.sos_token, vocab.eos_token]:
            summary_tokens.append(token)

    # 7. æ‹¼æ¥ä¸ºå®Œæ•´æ‘˜è¦å­—ç¬¦ä¸²
    return ' '.join(summary_tokens)


def collect_input_files(input_path: str) -> List[Path]:
    """æ”¶é›†è¾“å…¥è·¯å¾„ä¸‹çš„æ‰€æœ‰ .txt æ–‡ä»¶ï¼ˆæ”¯æŒå•æ–‡ä»¶æˆ–ç›®å½•ï¼‰"""
    input_path_obj = Path(input_path)
    if not input_path_obj.exists():
        raise FileNotFoundError(f"è¾“å…¥è·¯å¾„ä¸å­˜åœ¨ï¼š{input_path}")
    
    if input_path_obj.is_file():
        if input_path_obj.suffix == '.txt':
            return [input_path_obj]
        else:
            raise ValueError(f"è¾“å…¥æ–‡ä»¶å¿…é¡»æ˜¯ .txt æ ¼å¼ï¼ˆå½“å‰ï¼š{input_path_obj.suffix}ï¼‰")
    
    if input_path_obj.is_dir():
        # é€’å½’æŸ¥æ‰¾ç›®å½•ä¸‹æ‰€æœ‰ .txt æ–‡ä»¶ï¼ŒæŒ‰è·¯å¾„æ’åºç¡®ä¿ç»“æœç¨³å®š
        txt_files = sorted(list(input_path_obj.rglob('*.txt')))
        if not txt_files:
            raise FileNotFoundError(f"ç›®å½•ä¸‹æœªæ‰¾åˆ°ä»»ä½• .txt æ–‡ä»¶ï¼š{input_path}")
        return txt_files

    raise TypeError(f"è¾“å…¥è·¯å¾„å¿…é¡»æ˜¯æ–‡ä»¶æˆ–ç›®å½•ï¼ˆå½“å‰ï¼š{input_path_obj.stat().st_mode}ï¼‰")


def main(args):
    # è®°å½•æ¨ç†å¼€å§‹æ—¶é—´
    start_time = time.time()

    # 1. è®¾å¤‡åˆå§‹åŒ–ï¼ˆè‡ªåŠ¨æ£€æµ‹ GPU/CPUï¼Œä¼˜å…ˆç”¨ GPUï¼‰
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"âœ… ä½¿ç”¨è®¾å¤‡ï¼š{device}")

    # 2. åŠ è½½é…ç½®æ–‡ä»¶ï¼ˆè‹¥æŒ‡å®šï¼‰
    config = {}
    if args.config:
        config_path = Path(args.config)
        if not config_path.exists():
            raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼š{args.config}")
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        print(f"âœ… åŠ è½½é…ç½®æ–‡ä»¶ï¼š{args.config}")

    # 3. åŠ è½½è¯è¡¨ï¼ˆå¿…é¡»ä¸è®­ç»ƒæ—¶ä½¿ç”¨çš„è¯è¡¨ä¸€è‡´ï¼‰
    vocab_path = args.vocab_path or config.get('data', {}).get('vocab_path', '../data/processed/vocab.json')
    vocab_path_obj = Path(vocab_path)
    if not vocab_path_obj.exists():
        raise FileNotFoundError(f"è¯è¡¨æ–‡ä»¶ä¸å­˜åœ¨ï¼š{vocab_path}")
    vocab = Vocab.load(str(vocab_path_obj))
    print(f"âœ… åŠ è½½è¯è¡¨ï¼š{vocab_path}ï¼ˆè¯è¡¨å¤§å°ï¼š{len(vocab)}ï¼‰")

    # 4. åŠ è½½ PGCT æ¨¡å‹ï¼ˆæ ¸å¿ƒæ­¥éª¤ï¼‰
    checkpoint_path = args.checkpoint or config.get('train', {}).get('save_dir', '../checkpoints_pgct/best_model.pt')
    checkpoint_path_obj = Path(checkpoint_path)
    if not checkpoint_path_obj.exists():
        raise FileNotFoundError(f"æ¨¡å‹ checkpoint ä¸å­˜åœ¨ï¼š{checkpoint_path}")
    model, model_cfg = load_pgct_model(
        checkpoint_path=str(checkpoint_path_obj),
        vocab_size=len(vocab),
        pad_idx=vocab.pad_idx,
        device=device,
        config=config
    )
    print(f"âœ… åŠ è½½æ¨¡å‹ï¼š{checkpoint_path}")
    print(f"  - æ¨¡å‹å‚æ•°ï¼šhidden_size={model_cfg['hidden_size']}, encoder_layers={model_cfg['num_encoder_layers']}")
    print(f"  - æ¨ç†é…ç½®ï¼šmax_src_len={model_cfg['max_src_len']}, max_tgt_len={model_cfg['max_tgt_len']}")

    # 5. æ”¶é›†è¾“å…¥æ–‡ä»¶ï¼ˆå•æ–‡ä»¶æˆ–ç›®å½•ä¸‹æ‰€æœ‰ .txtï¼‰
    try:
        input_files = collect_input_files(args.input)
    except (FileNotFoundError, ValueError, TypeError) as e:
        print(f"âŒ è¾“å…¥æ–‡ä»¶å¤„ç†å¤±è´¥ï¼š{e}")
        sys.exit(1)
    print(f"âœ… æ”¶é›†åˆ°è¾“å…¥æ–‡ä»¶ï¼š{len(input_files)} ä¸ª")
    for i, file in enumerate(input_files[:3], 1):  # æ‰“å°å‰ 3 ä¸ªæ–‡ä»¶ç¤ºä¾‹ï¼ˆé¿å…è¾“å‡ºè¿‡é•¿ï¼‰
        print(f"  {i}. {file.name}ï¼ˆè·¯å¾„ï¼š{str(file.parent)[:50]}...ï¼‰")
    if len(input_files) > 3:
        print(f"  ... è¿˜æœ‰ {len(input_files)-3} ä¸ªæ–‡ä»¶æœªæ˜¾ç¤º")

    # 6. åˆ›å»ºè¾“å‡ºç›®å½•ï¼ˆç¡®ä¿è¾“å‡ºè·¯å¾„å­˜åœ¨ï¼Œé¿å…ä¿å­˜å¤±è´¥ï¼‰
    output_path_obj = Path(args.output)
    output_path_obj.parent.mkdir(parents=True, exist_ok=True)  # é€’å½’åˆ›å»ºçˆ¶ç›®å½•

    # 7. æ‰¹é‡ç”Ÿæˆæ‘˜è¦ï¼ˆæ ¸å¿ƒæ¨ç†å¾ªç¯ï¼‰
    results = []  # å­˜å‚¨æ‰€æœ‰æ¨ç†ç»“æœï¼ˆç”¨äºåç»­ä¿å­˜ï¼‰
    print(f"\nğŸš€ å¼€å§‹æ‰¹é‡ç”Ÿæˆæ‘˜è¦ï¼ˆè§£ç ç­–ç•¥ï¼š{args.decode_strategy}ï¼ŒæŸå¤§å°ï¼š{args.beam_size}ï¼‰")
    for idx, file in enumerate(input_files, 1):
        # è¯»å–è¾“å…¥æ–‡æœ¬ï¼ˆå‡è®¾ .txt æ–‡ä»¶ç›´æ¥å­˜å‚¨çº¯æ–‡ç« å†…å®¹ï¼Œæ— ç‰¹æ®Šæ ¼å¼ï¼‰
        try:
            with open(file, 'r', encoding='utf-8') as f:
                article = f.read().strip()
            if not article:
                print(f"âš ï¸ è·³è¿‡ç©ºæ–‡ä»¶ [{idx}/{len(input_files)}]ï¼š{file.name}")
                continue
        except Exception as e:
            print(f"âš ï¸ è¯»å–æ–‡ä»¶å¤±è´¥ [{idx}/{len(input_files)}]ï¼š{file.name}ï¼Œé”™è¯¯ï¼š{str(e)[:30]}...ï¼Œè·³è¿‡è¯¥æ–‡ä»¶")
            continue

        # è°ƒç”¨å‡½æ•°ç”Ÿæˆæ‘˜è¦
        try:
            summary = summarize_single_text(
                model=model,
                vocab=vocab,
                article=article,
                device=device,
                max_src_len=model_cfg['max_src_len'],
                max_tgt_len=model_cfg['max_tgt_len'],
                decode_strategy=args.decode_strategy,
                beam_size=args.beam_size
            )
        except Exception as e:
            print(f"âš ï¸ ç”Ÿæˆæ‘˜è¦å¤±è´¥ [{idx}/{len(input_files)}]ï¼š{file.name}ï¼Œé”™è¯¯ï¼š{str(e)[:30]}...ï¼Œè·³è¿‡è¯¥æ–‡ä»¶")
            continue

        # è®°å½•ç»“æœï¼ˆåŒ…å«å…³é”®ä¿¡æ¯ï¼Œä¾¿äºåç»­åˆ†æï¼‰
        results.append({
            'id': idx,
            'file_name': file.name,
            'file_path': str(file),
            'article_char_count': len(article),  # æ–‡ç« å­—ç¬¦æ•°
            'summary_token_count': len(summary.split()),  # æ‘˜è¦è¯æ•°
            'summary': summary
        })

        # æ‰“å°å®æ—¶è¿›åº¦ï¼ˆæ‘˜è¦é¢„è§ˆé™åˆ¶ 150 å­—ç¬¦ï¼Œé¿å…è¾“å‡ºè¿‡é•¿ï¼‰
        summary_preview = summary[:150] + "..." if len(summary) > 150 else summary
        print(f"âœ… å®Œæˆ [{idx}/{len(input_files)}] | æ–‡ä»¶ï¼š{file.name} | æ‘˜è¦è¯æ•°ï¼š{len(summary.split())}")
        print(f"   æ‘˜è¦ï¼š{summary_preview}")

    # 8. ä¿å­˜æ¨ç†ç»“æœåˆ°æ–‡ä»¶ï¼ˆæ–‡æœ¬æ ¼å¼ï¼Œä¾¿äºé˜…è¯»å’Œåç»­åˆ†æï¼‰
    with open(output_path_obj, 'w', encoding='utf-8') as f:
        # å†™å…¥å¤´éƒ¨ä¿¡æ¯ï¼ˆé…ç½®+ç»Ÿè®¡ï¼‰
        f.write("="*60 + "\n")
        f.write("PGCT æ¨¡å‹æ¨ç†ç»“æœæ±‡æ€»\n")
        f.write("="*60 + "\n")
        f.write(f"æ¨ç†é…ç½®ï¼š\n")
        f.write(f"  - æ¨¡å‹è·¯å¾„ï¼š{checkpoint_path}\n")
        f.write(f"  - è¯è¡¨è·¯å¾„ï¼š{vocab_path}\n")
        f.write(f"  - è§£ç ç­–ç•¥ï¼š{args.decode_strategy}\n")
        f.write(f"  - æŸæœç´¢å¤§å°ï¼š{args.beam_size}\n")
        f.write(f"  - æœ€å¤§æºæ–‡æœ¬é•¿åº¦ï¼š{model_cfg['max_src_len']}\n
