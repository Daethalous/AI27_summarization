"""
PGCT æ¨¡å‹æ¨ç†è„šæœ¬ï¼ˆå•æ¡æˆ–æ‰¹é‡æ–‡æœ¬æ–‡ä»¶ï¼‰
é€‚é… Transformer + Pointer-Generator + Coverage æ¨¡å‹
æ”¯æŒ OOV è¯å¤„ç†ä¸ Greedy/Beam è§£ç 
"""
import os
import sys
import time
from pathlib import Path
import argparse
from typing import List, Dict

import torch
import yaml
import nltk
from nltk.tokenize import word_tokenize

try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

# å°† src ç›®å½•æ·»åŠ åˆ° Python è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from models.pgct.pgct_model import PGCTModel
from models.pgct.pgct_decoding import pgct_greedy_decode, pgct_beam_search_decode
from utils.vocab import Vocab


def tokenize(text: str, lowercase: bool = True) -> List[str]:
    """ä½¿ç”¨ nltk.word_tokenize å¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯å’Œå¯é€‰çš„å°å†™åŒ–"""
    if lowercase:
        text = text.lower()
    return word_tokenize(text)


def process_oov(tokens: List[str], vocab: Vocab) -> tuple[List[int], Dict[int, str], List[int]]:
    """å¤„ç† OOV è¯ï¼Œè¿”å›è¯ç´¢å¼•ã€OOV è¯å…¸å’Œ OOV æ˜ å°„"""
    src_indices, oov_dict, src_oov_map = [], {}, []
    for token in tokens:
        if token in vocab.word2idx:
            src_indices.append(vocab.word2idx[token])
            src_oov_map.append(-1) # -1 è¡¨ç¤ºè¯åœ¨åŸºç¡€è¯è¡¨ä¸­
        else:
            # è¿™æ˜¯ä¸€ä¸ª OOV è¯
            if token not in oov_dict.values():
                # åˆ†é…ä¸€ä¸ªæ–°çš„ OOV ç›¸å¯¹ç´¢å¼•
                new_oov_idx = len(oov_dict)
                oov_dict[new_oov_idx] = token
            
            # è·å– OOV ç›¸å¯¹ç´¢å¼•
            oov_idx = [k for k, v in oov_dict.items() if v == token][0]
            
            src_indices.append(vocab.unk_idx) # ä½¿ç”¨ <unk> ç´¢å¼•å ä½
            src_oov_map.append(oov_idx)       # è®°å½• OOV è¯åœ¨æ‰©å±•è¯è¡¨ä¸­çš„ç›¸å¯¹ä½ç½®
    return src_indices, oov_dict, src_oov_map


# [MODIFIED] ä¿®å¤äº†å‚æ•°ç±»å‹è½¬æ¢é—®é¢˜
def load_pgct_model(checkpoint_path: str, vocab_size: int, pad_idx: int, device: torch.device, config: Dict = None) -> tuple[PGCTModel, Dict]:
    checkpoint = torch.load(checkpoint_path, map_location=device)
    cfg = config.get('model', {}) if config else {}
    # ä» Checkpoint ä¸­è·å–è®­ç»ƒæ—¶çš„æ¨¡å‹é…ç½® (ä¼˜å…ˆçº§æœ€ä½)
    checkpoint_model_cfg = checkpoint.get('config', {}).get('model', {}) 
    
    # è¾…åŠ©å‡½æ•°ï¼šå®‰å…¨åœ°è·å–æ•´æ•°æˆ–æµ®ç‚¹æ•°å‚æ•°
    def safe_get_param(key, default_val, is_int=True):
        # ä¼˜å…ˆçº§ï¼šé…ç½®æ–‡ä»¶ > Checkpointé…ç½® > ç¡¬ç¼–ç é»˜è®¤å€¼
        val = cfg.get(key)
        if val is None:
             val = checkpoint_model_cfg.get(key)
             if val is None:
                 val = default_val
        
        try:
            # å¼ºåˆ¶è½¬æ¢ä¸º int æˆ– float
            return int(val) if is_int else float(val)
        except (TypeError, ValueError):
            # æŠ¥å‘Šé”™è¯¯å¹¶è¿”å›é»˜è®¤å€¼
            print(f"âš ï¸ æ¨¡å‹å‚æ•° {key} ('{val}') è¯»å–æˆ–è½¬æ¢å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼ {default_val}ã€‚")
            return default_val

    # æ„é€ æ¨¡å‹é…ç½®å­—å…¸ï¼Œå¹¶ç¡®ä¿æ‰€æœ‰å€¼éƒ½æ˜¯ int æˆ– float
    model_cfg = {
        'embed_size': safe_get_param('embed_size', 512),
        'hidden_size': safe_get_param('hidden_size', 512),
        'num_encoder_layers': safe_get_param('num_encoder_layers', 3),
        'num_decoder_layers': safe_get_param('num_decoder_layers', 3),
        'nhead': safe_get_param('nhead', 8),
        'dropout': safe_get_param('dropout', 0.1, is_int=False), # æµ®ç‚¹æ•°
        'cov_loss_weight': safe_get_param('cov_loss_weight', 1.0, is_int=False), # æµ®ç‚¹æ•°
        'max_src_len': safe_get_param('max_src_len', 400),
        'max_tgt_len': safe_get_param('max_tgt_len', 100)
    }

    model = PGCTModel(
        vocab_size=vocab_size,
        embed_size=model_cfg['embed_size'],
        hidden_size=model_cfg['hidden_size'],
        num_encoder_layers=model_cfg['num_encoder_layers'],
        num_decoder_layers=model_cfg['num_decoder_layers'],
        nhead=model_cfg['nhead'],
        dropout=model_cfg['dropout'],
        pad_idx=pad_idx,
        cov_loss_weight=model_cfg['cov_loss_weight'],
        max_src_len=model_cfg['max_src_len'],
        max_tgt_len=model_cfg['max_tgt_len']
    ).to(device)

    if 'model_state_dict' in checkpoint:
        model.load_state_dict(checkpoint['model_state_dict'])
    else:
        # å…¼å®¹ç›´æ¥ä¿å­˜ state_dict çš„æƒ…å†µ (ä¾‹å¦‚ best_model.pt)
        model.load_state_dict(checkpoint)

    model.eval()
    return model, model_cfg


def summarize_single_text(model: PGCTModel, vocab: Vocab, article: str, device: torch.device, max_src_len: int, max_tgt_len: int, decode_strategy: str, beam_size: int) -> str:
    """å¯¹å•ç¯‡æ–‡æœ¬è¿›è¡Œæ‘˜è¦ç”Ÿæˆ"""
    tokens = tokenize(article)[:max_src_len]
    src_len = len(tokens)
    src_indices, oov_dict, src_oov_map = process_oov(tokens, vocab)

    # å¡«å……åˆ°æœ€å¤§æºé•¿åº¦
    pad_len = max_src_len - len(src_indices)
    src_indices += [vocab.pad_idx] * pad_len
    src_oov_map += [-1] * pad_len

    # è½¬æ¢ä¸º Tensor
    src_tensor = torch.LongTensor([src_indices]).to(device)
    src_len_tensor = torch.LongTensor([src_len]).to(device)
    src_oov_tensor = torch.LongTensor([src_oov_map]).to(device)

    with torch.no_grad():
        if decode_strategy == 'beam':
            pred_ids, _ = pgct_beam_search_decode(
                model, src_tensor, src_len_tensor, src_oov_tensor, 
                max_tgt_len, vocab.sos_idx, vocab.eos_idx, beam_size, device
            )
        else:
            pred_ids, _ = pgct_greedy_decode(
                model, src_tensor, src_len_tensor, src_oov_tensor, 
                max_tgt_len, vocab.sos_idx, vocab.eos_idx, device
            )

    summary_tokens = []
    # è§£ç ç»“æœè½¬æ¢ä¸ºæ–‡æœ¬
    for idx in pred_ids.squeeze().tolist():
        idx_val = idx if isinstance(idx, int) else idx.item()
        if idx_val < len(vocab):
            # åŸºç¡€è¯è¡¨ä¸­çš„è¯
            token = vocab.idx2word.get(idx_val, vocab.UNK_TOKEN)
        else:
            # æ‰©å±•è¯è¡¨ä¸­çš„ OOV è¯
            oov_rel_idx = idx_val - len(vocab)
            token = oov_dict.get(oov_rel_idx, vocab.UNK_TOKEN)
        
        # è¿‡æ»¤ç‰¹æ®Š token
        if token not in [vocab.PAD_TOKEN, vocab.SOS_TOKEN, vocab.EOS_TOKEN]:
            summary_tokens.append(token)

    return ' '.join(summary_tokens)


def collect_input_files(input_path: str) -> List[Path]:
    """æ”¶é›†è¾“å…¥è·¯å¾„ä¸‹çš„æ‰€æœ‰ .txt æ–‡ä»¶"""
    path = Path(input_path)
    if not path.exists():
        raise FileNotFoundError(f"è¾“å…¥è·¯å¾„ä¸å­˜åœ¨ï¼š{input_path}")
    if path.is_file() and path.suffix == '.txt':
        return [path]
    elif path.is_dir():
        txt_files = sorted(list(path.rglob('*.txt')))
        if not txt_files:
            raise FileNotFoundError(f"ç›®å½•ä¸‹æœªæ‰¾åˆ°ä»»ä½• .txt æ–‡ä»¶ï¼š{input_path}")
        return txt_files
    else:
        raise ValueError(f"è¾“å…¥è·¯å¾„å¿…é¡»æ˜¯ .txt æ–‡ä»¶æˆ–ç›®å½•ï¼š{input_path}")


def main(args):
    start_time = time.time() # è®°å½•å¼€å§‹æ—¶é—´

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"âœ… ä½¿ç”¨è®¾å¤‡ï¼š{device}")

    config = {}
    if args.config:
        try:
            with open(args.config, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            print(f"âœ… åŠ è½½é…ç½®æ–‡ä»¶ï¼š{args.config}")
        except Exception as e:
            print(f"âš ï¸ é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å‚æ•°ã€‚é”™è¯¯ï¼š{e}")

    data_cfg = config.get('data', {})
    eval_cfg = config.get('eval', {})

    # -------------------------------------------------------------------------
    # å…³é”®ä¿®æ”¹1ï¼šä¿®æ­£ vocab_path ä¼˜å…ˆçº§ï¼Œå¼ºåˆ¶è½¬ä¸ºç»å¯¹è·¯å¾„
    # ä¼˜å…ˆçº§ï¼šå‘½ä»¤è¡Œè¾“å…¥ > é…ç½®æ–‡ä»¶ > é»˜è®¤å€¼
    vocab_path = args.vocab_path  # ä¼˜å…ˆä½¿ç”¨ç”¨æˆ·å‘½ä»¤è¡Œä¼ å…¥çš„è·¯å¾„
    if not vocab_path:  # è‹¥ç”¨æˆ·æœªä¼ å…¥ï¼ˆNone æˆ–ç©ºå­—ç¬¦ä¸²ï¼‰ï¼Œä»é…ç½®æ–‡ä»¶è¯»å–
        vocab_path = data_cfg.get('vocab_path')
    if not vocab_path:  # æœ€åä½¿ç”¨é»˜è®¤ç›¸å¯¹è·¯å¾„ï¼ˆä» src ç›®å½•å‡ºå‘ï¼‰
        vocab_path = '../data/processed/vocab.json'
    
    # å¼ºåˆ¶è½¬ä¸ºç»å¯¹è·¯å¾„ï¼ˆå½»åº•è§£å†³ç›¸å¯¹è·¯å¾„æ­§ä¹‰ï¼‰
    vocab_path = Path(vocab_path).resolve()
    print(f"ğŸ“Œ æœ€ç»ˆè¯è¡¨è·¯å¾„ï¼ˆç»å¯¹è·¯å¾„ï¼‰ï¼š{vocab_path}")
    print(f"ğŸ“Œ è¯è¡¨æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼š{vocab_path.exists()}")
    # -------------------------------------------------------------------------

    # å¤„ç† checkpoint è·¯å¾„ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼Œå¢åŠ ç»å¯¹è·¯å¾„è½¬æ¢ï¼‰
    checkpoint_path = args.checkpoint or config.get('train', {}).get('save_dir', '../checkpoints_pgct')
    if Path(checkpoint_path).is_dir():
         checkpoint_path = str(Path(checkpoint_path) / 'best_model.pt')
    checkpoint_path = Path(checkpoint_path).resolve()  # ä¹Ÿè½¬ä¸ºç»å¯¹è·¯å¾„ï¼Œé¿å…æ¨¡å‹åŠ è½½å¤±è´¥

    input_path = args.input
    decode_strategy = args.decode_strategy or eval_cfg.get('decode_strategy', 'greedy')
    beam_size = args.beam_size or eval_cfg.get('beam_size', 5)
    
    # ç»„åˆè¾“å‡ºè·¯å¾„ï¼ˆè½¬ä¸ºç»å¯¹è·¯å¾„ï¼‰
    output_dir = eval_cfg.get('output_dir', '../outputs_pgct')
    output_name = eval_cfg.get('output_file', 'test_summaries.txt')
    output_file = args.output or os.path.join(output_dir, output_name)
    output_file = Path(output_file).resolve()

    # -------------------------------------------------------------------------
    # å…³é”®ä¿®æ”¹2ï¼šåŠ è½½è¯è¡¨å‰éªŒè¯è·¯å¾„ï¼Œæ˜ç¡®æŠ¥é”™ä¿¡æ¯
    try:
        if not vocab_path.exists():
            raise FileNotFoundError(f"è¯è¡¨æ–‡ä»¶ä¸å­˜åœ¨ï¼ˆç»å¯¹è·¯å¾„ï¼š{vocab_path}ï¼‰")
        # ç¡®ä¿ä¼ å…¥å­—ç¬¦ä¸²è·¯å¾„ï¼ˆå…¼å®¹ Vocab.load å¯èƒ½çš„æ ¼å¼è¦æ±‚ï¼‰
        vocab = Vocab.load(str(vocab_path))
        print(f"âœ… åŠ è½½è¯è¡¨ï¼š{vocab_path}ï¼ˆå¤§å°ï¼š{len(vocab)}ï¼‰")
    except Exception as e:
        print(f"âŒ è¯è¡¨åŠ è½½å¤±è´¥ï¼š{e}ã€‚è¯·æ£€æŸ¥ï¼š1. è·¯å¾„æ˜¯å¦æ­£ç¡®ï¼›2. æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼›3. æ–‡ä»¶æ ¼å¼æ˜¯å¦ä¸º valid JSONã€‚")
        return
    # -------------------------------------------------------------------------

    # åŠ è½½æ¨¡å‹ï¼ˆä½¿ç”¨ç»å¯¹è·¯å¾„ï¼‰
    try:
        model, model_cfg = load_pgct_model(str(checkpoint_path), len(vocab), vocab.pad_idx, device, config)
        print(f"âœ… åŠ è½½æ¨¡å‹ï¼š{checkpoint_path}")
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½æˆ–åˆå§‹åŒ–å¤±è´¥ã€‚é”™è¯¯ï¼š{e}")
        return

    # æ”¶é›†è¾“å…¥æ–‡ä»¶
    try:
        input_files = collect_input_files(input_path)
        print(f"âœ… æ”¶é›†åˆ°è¾“å…¥æ–‡ä»¶ï¼š{len(input_files)} ä¸ª")
    except Exception as e:
        print(f"âŒ è¾“å…¥æ–‡ä»¶æ”¶é›†å¤±è´¥ï¼š{e}")
        return

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_file.parent.mkdir(parents=True, exist_ok=True)

    results = []
    print(f"\nğŸš€ å¼€å§‹æ‰¹é‡ç”Ÿæˆæ‘˜è¦ï¼ˆç­–ç•¥ï¼š{decode_strategy}ï¼ŒæŸå¤§å°ï¼š{beam_size}ï¼‰")
    for idx, file in enumerate(input_files, 1):
        # è¿›åº¦æŒ‡ç¤ºå™¨
        print(f"\rå¤„ç†è¿›åº¦ï¼š[{idx}/{len(input_files)}]", end="", flush=True)

        try:
            with open(file, 'r', encoding='utf-8') as f:
                article = f.read().strip()
            if not article:
                continue
        except Exception as e:
            print(f"\nâš ï¸ è¯»å–å¤±è´¥ [{idx}/{len(input_files)}]ï¼š{file.name}ï¼Œé”™è¯¯ï¼š{str(e)[:50]}...ï¼Œè·³è¿‡")
            continue

        try:
            summary = summarize_single_text(
                model=model,
                vocab=vocab,
                article=article,
                device=device,
                max_src_len=model_cfg['max_src_len'],
                max_tgt_len=model_cfg['max_tgt_len'],
                decode_strategy=decode_strategy,
                beam_size=beam_size
            )
        except Exception as e:
            print(f"\nâš ï¸ ç”Ÿæˆæ‘˜è¦å¤±è´¥ [{idx}/{len(input_files)}]ï¼š{file.name}ï¼Œé”™è¯¯ï¼š{str(e)[:50]}...ï¼Œè·³è¿‡è¯¥æ–‡ä»¶")
            continue

        results.append({
            'id': idx,
            'file_name': file.name,
            'file_path': str(file),
            'article_char_count': len(article),
            'summary_token_count': len(summary.split()),
            'summary': summary
        })

    # é¢„è§ˆæœ€åä¸€ä¸ªç”Ÿæˆç»“æœ
    if results:
        last_item = results[-1]
        summary = last_item['summary']
        preview = summary[:150] + "..." if len(summary) > 150 else summary
        print(f"\n\nâœ… æœ€åä¸€ä¸ªå®Œæˆ | æ–‡ä»¶ï¼š{last_item['file_name']} | æ‘˜è¦è¯æ•°ï¼š{last_item['summary_token_count']}")
        print(f"   æ‘˜è¦é¢„è§ˆï¼š{preview}")

    # ä¿å­˜ç»“æœ
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write("=" * 60 + "\n")
        f.write("PGCT æ¨¡å‹æ¨ç†ç»“æœæ±‡æ€»\n")
        f.write("=" * 60 + "\n")
        f.write(f"æ¨ç†é…ç½®ï¼š\n")
        f.write(f"  - æ¨¡å‹è·¯å¾„ï¼š{checkpoint_path}\n")
        f.write(f"  - è¯è¡¨è·¯å¾„ï¼š{vocab_path}\n")
        f.write(f"  - è§£ç ç­–ç•¥ï¼š{decode_strategy}\n")
        f.write(f"  - æŸæœç´¢å¤§å°ï¼š{beam_size}\n")
        f.write(f"  - æœ€å¤§æºæ–‡æœ¬é•¿åº¦ï¼š{model_cfg['max_src_len']}\n")
        f.write(f"  - æœ€å¤§æ‘˜è¦é•¿åº¦ï¼š{model_cfg['max_tgt_len']}\n")
        f.write(f"  - æˆåŠŸå¤„ç†æ–‡ä»¶æ•°ï¼š{len(results)}\n")
        f.write("=" * 60 + "\n\n")

        for item in results:
            f.write(f"=== æ ·æœ¬ {item['id']} ===\n")
            f.write(f"æ–‡ä»¶åï¼š{item['file_name']}\n")
            f.write(f"è·¯å¾„ï¼š{item['file_path']}\n")
            f.write(f"åŸæ–‡å­—ç¬¦æ•°ï¼š{item['article_char_count']}\n")
            f.write(f"æ‘˜è¦è¯æ•°ï¼š{item['summary_token_count']}\n")
            f.write(f"ç”Ÿæˆæ‘˜è¦ï¼š{item['summary']}\n\n")

    elapsed = time.time() - start_time
    print(f"\nâœ… æ¨ç†å®Œæˆï¼Œå…±å¤„ç† {len(results)} ä¸ªæ ·æœ¬ï¼Œæ€»ç”¨æ—¶ {elapsed:.2f} ç§’")
    print(f"âœ“ ç»“æœå·²ä¿å­˜åˆ°ï¼š{output_file}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="PGCT æ¨¡å‹æ¨ç†è„šæœ¬")
    parser.add_argument('--config', type=str, help='é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆå¦‚ ../configs/pgct.yamlï¼‰')
    parser.add_argument('--checkpoint', type=str, help='æ¨¡å‹ checkpoint è·¯å¾„')
    parser.add_argument('--vocab_path', type=str, help='è¯è¡¨ç»å¯¹è·¯å¾„ï¼ˆä¼˜å…ˆäºé…ç½®æ–‡ä»¶ï¼Œå¿…å¡«ï¼‰')
    parser.add_argument('--input', type=str, required=True, help='è¾“å…¥ .txt æ–‡ä»¶æˆ–ç›®å½•ï¼ˆå¦‚ ../data/raw/testï¼‰')
    parser.add_argument('--output', type=str, help='ç»“æœä¿å­˜ç»å¯¹è·¯å¾„ï¼ˆé»˜è®¤ä» config è¯»å–ï¼‰')
    parser.add_argument('--decode_strategy', type=str, choices=['greedy', 'beam'], help='è§£ç ç­–ç•¥ï¼ˆé»˜è®¤ä» config è¯»å–ï¼‰')
    parser.add_argument('--beam_size', type=int, help='æŸæœç´¢å¤§å°ï¼ˆä»… beam ç­–ç•¥ç”Ÿæ•ˆï¼‰')
    args = parser.parse_args()
    main(args)
