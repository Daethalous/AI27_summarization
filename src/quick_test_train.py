"""
å¿«é€Ÿæµ‹è¯•è®­ç»ƒè„šæœ¬ - åªä½¿ç”¨å°‘é‡æ•°æ®éªŒè¯ä»£ç èƒ½æ­£å¸¸è¿è¡Œ

ä½¿ç”¨æ–¹æ³•:
    python quick_test_train.py --model baseline   # æµ‹è¯•baselineæ¨¡å‹
    python quick_test_train.py --model pg         # æµ‹è¯•pointer-generatoræ¨¡å‹
    python quick_test_train.py --model pg_cov     # æµ‹è¯•pointer-generator with coverageæ¨¡å‹ (æ–°å¢)
"""
from __future__ import annotations

import argparse
import logging
import sys
from pathlib import Path

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Subset
from tqdm import tqdm

# æ·»åŠ srcåˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))
# è‡ªåŠ¨å®šä½é¡¹ç›®æ ¹ç›®å½•ï¼ˆå‡è®¾å½“å‰æ–‡ä»¶æ˜¯ src/quick_test_train.pyï¼‰
project_root = Path(__file__).resolve().parents[1]
default_data_dir = project_root / 'data' / 'raw'


from datamodules.cnndm import prepare_datasets, get_dataloader
from models.baseline.model import Seq2Seq
from models.pointer_generator import PointerGeneratorSeq2Seq
# =======================================================
# ğŸš€ å…³é”®ä¿®æ”¹ 1: å¯¼å…¥ PG with Coverage æ¨¡å‹
from models.pointer_generator_coverage.pg_coverage_model import PGCoverageSeq2Seq
# =======================================================
from utils.vocab import Vocab
from utils.metrics import batch_compute_metrics


def setup_logger():
    """è®¾ç½®ç®€å•çš„æ—¥å¿—"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    return logging.getLogger(__name__)


def quick_test_baseline(args):
    """å¿«é€Ÿæµ‹è¯•baselineæ¨¡å‹"""
    logger = setup_logger()
    logger.info("=" * 50)
    logger.info("å¿«é€Ÿæµ‹è¯• Baseline æ¨¡å‹ï¼ˆSeq2Seq + Attentionï¼‰")
    logger.info("=" * 50)

    # è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")

    # 1. å‡†å¤‡æ•°æ®ï¼ˆåªä½¿ç”¨å¾ˆå°‘çš„æ ·æœ¬ï¼‰
    logger.info("\n[1/5] å‡†å¤‡æ•°æ®...")
    data_dir = Path(args.data_dir)
    processed_dir = data_dir.parent / 'processed'

    # å‡†å¤‡æ•°æ®é›†ï¼ˆå¦‚æœéœ€è¦ï¼‰
    vocab_path = processed_dir / 'vocab.json'
    if not vocab_path.exists():
        logger.info("è¯è¡¨ä¸å­˜åœ¨ï¼Œå¼€å§‹é¢„å¤„ç†æ•°æ®...")
        # =======================================================
        # ğŸ› ä¿®å¤ prepare_datasets è°ƒç”¨å‚æ•°
        # =======================================================
        prepare_datasets(
            raw_dir=str(data_dir),
            processed_dir=str(processed_dir), 
            vocab_path=str(vocab_path),      
            max_vocab_size=args.max_vocab_size,
            min_freq=args.min_freq,
            max_src_len=args.max_src_len,
            max_tgt_len=args.max_tgt_len
        )
        # =======================================================

    # åŠ è½½è¯è¡¨
    vocab = Vocab.load(str(vocab_path))
    logger.info(f"è¯è¡¨å¤§å°: {len(vocab)}")

    # åŠ è½½æ•°æ®ï¼ˆä½¿ç”¨å°æ‰¹é‡ï¼‰
    train_loader = get_dataloader(
        processed_dir=str(processed_dir),
        batch_size=args.batch_size,
        split='train',
        shuffle=True
    )

    val_loader = get_dataloader(
        processed_dir=str(processed_dir),
        batch_size=args.batch_size,
        split='val',
        shuffle=False
    )

    # åªä½¿ç”¨å°‘é‡æ•°æ®
    train_subset_indices = list(range(min(args.num_samples, len(train_loader.dataset))))
    train_subset = Subset(train_loader.dataset, train_subset_indices)
    train_loader_small = DataLoader(
        train_subset,
        batch_size=args.batch_size,
        shuffle=True,
        collate_fn=train_loader.collate_fn
    )

    val_subset_indices = list(range(min(args.num_samples // 2, len(val_loader.dataset))))
    val_subset = Subset(val_loader.dataset, val_subset_indices)
    val_loader_small = DataLoader(
        val_subset,
        batch_size=args.batch_size,
        shuffle=False,
        collate_fn=val_loader.collate_fn
    )

    logger.info(f"è®­ç»ƒæ ·æœ¬æ•°: {len(train_subset)}")
    logger.info(f"éªŒè¯æ ·æœ¬æ•°: {len(val_subset)}")

    # 2. åˆ›å»ºæ¨¡å‹
    logger.info("\n[2/5] åˆ›å»ºæ¨¡å‹...")
    model = Seq2Seq(
        vocab_size=len(vocab),
        embed_size=args.embed_size,
        hidden_size=args.hidden_size,
        num_layers=args.num_layers,
        dropout=args.dropout,
        pad_idx=vocab.word2idx['<PAD>']
    ).to(device)

    logger.info(f"æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")

    # 3. ä¼˜åŒ–å™¨
    logger.info("\n[3/5] åˆ›å»ºä¼˜åŒ–å™¨...")
    optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)
    criterion = nn.CrossEntropyLoss(ignore_index=vocab.word2idx['<PAD>'])

    # 4. è®­ç»ƒå‡ ä¸ªepoch
    logger.info(f"\n[4/5] å¼€å§‹è®­ç»ƒ {args.num_epochs} ä¸ªepoch...")

    for epoch in range(args.num_epochs):
        model.train()
        total_loss = 0

        pbar = tqdm(train_loader_small, desc=f"Epoch {epoch+1}/{args.num_epochs}")
        for batch in pbar:
            src = batch['src'].to(device)
            tgt = batch['tgt'].to(device)

            optimizer.zero_grad()

            # å‰å‘ä¼ æ’­
            output = model(src, tgt[:, :-1])  # teacher forcing

            # è®¡ç®—æŸå¤±
            output_flat = output.reshape(-1, output.size(-1))
            tgt_flat = tgt[:, 1:].reshape(-1)
            loss = criterion(output_flat, tgt_flat)

            # åå‘ä¼ æ’­
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)
            optimizer.step()

            total_loss += loss.item()
            pbar.set_postfix({'loss': f'{loss.item():.4f}'})

        avg_loss = total_loss / len(train_loader_small)
        logger.info(f"Epoch {epoch+1} è®­ç»ƒæŸå¤±: {avg_loss:.4f}")

        # éªŒè¯
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for batch in val_loader_small:
                src = batch['src'].to(device)
                tgt = batch['tgt'].to(device)

                output = model(src, tgt[:, :-1])
                output_flat = output.reshape(-1, output.size(-1))
                tgt_flat = tgt[:, 1:].reshape(-1)
                loss = criterion(output_flat, tgt_flat)
                val_loss += loss.item()

        avg_val_loss = val_loss / len(val_loader_small)
        logger.info(f"Epoch {epoch+1} éªŒè¯æŸå¤±: {avg_val_loss:.4f}")

    # 5. æµ‹è¯•æ¨ç†
    logger.info("\n[5/5] æµ‹è¯•æ¨ç†åŠŸèƒ½...")
    model.eval()
    with torch.no_grad():
        test_batch = next(iter(val_loader_small))
        src = test_batch['src'][:1].to(device)  # åªå–ä¸€ä¸ªæ ·æœ¬

        # Greedy decoding
        predictions = model.greedy_decode(
            src,
            max_len=50,
            sos_idx=vocab.word2idx['<SOS>'],
            eos_idx=vocab.word2idx['<EOS>']
        )

        # è½¬æ¢ä¸ºæ–‡æœ¬
        pred_tokens = [vocab.idx2word.get(idx.item(), Vocab.UNK_TOKEN)
                      for idx in predictions[0]
                      if idx.item() not in [vocab.word2idx['<PAD>'],
                                           vocab.word2idx['<SOS>'],
                                           vocab.word2idx['<EOS>']]]

        logger.info(f"ç”Ÿæˆçš„æ‘˜è¦æ ·ä¾‹: {' '.join(pred_tokens[:20])}...")

    logger.info("\n" + "=" * 50)
    logger.info("âœ… æµ‹è¯•å®Œæˆï¼Baselineæ¨¡å‹è¿è¡Œæ­£å¸¸ï¼")
    logger.info("=" * 50)


def quick_test_pointer_generator(args):
    """å¿«é€Ÿæµ‹è¯•Pointer-Generatoræ¨¡å‹"""
    logger = setup_logger()
    logger.info("=" * 50)
    logger.info("å¿«é€Ÿæµ‹è¯• Pointer-Generator æ¨¡å‹")
    logger.info("=" * 50)

    # è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")

    # 1. å‡†å¤‡æ•°æ®
    logger.info("\n[1/5] å‡†å¤‡æ•°æ®...")
    data_dir = Path(args.data_dir)
    processed_dir = data_dir.parent / 'processed'

    vocab_path = processed_dir / 'vocab.json'
    if not vocab_path.exists():
        logger.info("è¯è¡¨ä¸å­˜åœ¨ï¼Œå¼€å§‹é¢„å¤„ç†æ•°æ®...")
        from datamodules.cnndm import prepare_datasets
        # =======================================================
        # ğŸ› ä¿®å¤ prepare_datasets è°ƒç”¨å‚æ•°
        # =======================================================
        prepare_datasets(
            raw_dir=str(data_dir),
            processed_dir=str(processed_dir), 
            vocab_path=str(vocab_path),      
            max_vocab_size=args.max_vocab_size,
            min_freq=args.min_freq,
            max_src_len=args.max_src_len,
            max_tgt_len=args.max_tgt_len
        )
        # =======================================================

    # åŠ è½½è¯è¡¨
    vocab = Vocab.load(str(vocab_path))
    logger.info(f"è¯è¡¨å¤§å°: {len(vocab)}")

    # åŠ è½½æ•°æ®
    train_loader = get_dataloader(
        processed_dir=str(processed_dir),
        batch_size=args.batch_size,
        split='train',
        shuffle=True
    )

    # åªä½¿ç”¨å°‘é‡æ•°æ®
    train_subset_indices = list(range(min(args.num_samples, len(train_loader.dataset))))
    train_subset = Subset(train_loader.dataset, train_subset_indices)
    train_loader_small = DataLoader(
        train_subset,
        batch_size=args.batch_size,
        shuffle=True,
        collate_fn=train_loader.collate_fn
    )

    logger.info(f"è®­ç»ƒæ ·æœ¬æ•°: {len(train_subset)}")

    # 2. åˆ›å»ºPointer-Generatoræ¨¡å‹
    logger.info("\n[2/5] åˆ›å»ºPointer-Generatoræ¨¡å‹...")
    model = PointerGeneratorSeq2Seq(
        vocab_size=len(vocab),
        embed_size=args.embed_size,
        hidden_size=args.hidden_size,
        num_layers=args.num_layers,
        dropout=args.dropout,
        max_oov_size=args.max_oov_size,
        pad_idx=vocab.word2idx['<PAD>']
    ).to(device)

    logger.info(f"æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")

    # 3. ä¼˜åŒ–å™¨
    logger.info("\n[3/5] åˆ›å»ºä¼˜åŒ–å™¨...")
    optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)

    # 4. è®­ç»ƒå‡ ä¸ªepoch
    logger.info(f"\n[4/5] å¼€å§‹è®­ç»ƒ {args.num_epochs} ä¸ªepoch...")

    for epoch in range(args.num_epochs):
        model.train()
        total_loss = 0

        pbar = tqdm(train_loader_small, desc=f"Epoch {epoch+1}/{args.num_epochs}")
        for batch in pbar:
            src = batch['src'].to(device)
            tgt = batch['tgt'].to(device)
            src_lens = batch.get('src_lens', None)
            src_oov_map = batch.get('src_oov_map', None)

            optimizer.zero_grad()

            # å‰å‘ä¼ æ’­: PGæ¨¡å‹è¿”å› 1 ä¸ªè¾“å‡º
            output = model(
                src=src,
                tgt=tgt,
                src_lens=src_lens,
                src_oov_map=src_oov_map
            )

            # è®¡ç®—æŸå¤±ï¼ˆå¤„ç†æ‰©å±•è¯è¡¨ï¼‰
            batch_size, seq_len, vocab_size = output.shape
            output_flat = output.reshape(-1, vocab_size)
            tgt_flat = tgt[:, 1:].reshape(-1)

            # è®¡ç®—è´Ÿå¯¹æ•°ä¼¼ç„¶
            log_probs = torch.log(output_flat + 1e-10)
            target_log_probs = log_probs.gather(1, tgt_flat.unsqueeze(1)).squeeze(1)

            # Mask padding
            mask = (tgt_flat != vocab.word2idx['<PAD>']).float()
            loss = -(target_log_probs * mask).sum() / mask.sum()

            # åå‘ä¼ æ’­
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)
            optimizer.step()

            total_loss += loss.item()
            pbar.set_postfix({'loss': f'{loss.item():.4f}'})

        avg_loss = total_loss / len(train_loader_small)
        logger.info(f"Epoch {epoch+1} è®­ç»ƒæŸå¤±: {avg_loss:.4f}")

    # 5. æµ‹è¯•æ¨ç†
    logger.info("\n[5/5] æµ‹è¯•æ¨ç†åŠŸèƒ½...")
    model.eval()
    with torch.no_grad():
        test_batch = next(iter(train_loader_small))
        src = test_batch['src'][:1].to(device)
        src_lens = test_batch.get('src_lens', None)
        if src_lens is not None:
            src_lens = src_lens[:1]
        src_oov_map = test_batch.get('src_oov_map', None)
        if src_oov_map is not None:
             src_oov_map = src_oov_map[:1].to(device)
        oov_lists = test_batch.get('oov_list', [[]])[:1]

        # Greedy decoding
        predictions, _ = model.generate(
            src=src,
            src_lens=src_lens,
            src_oov_map=src_oov_map,
            max_length=50,
            sos_idx=vocab.word2idx['<SOS>'],
            eos_idx=vocab.word2idx['<EOS>'],
        )

        # è½¬æ¢ä¸ºæ–‡æœ¬ï¼ˆå¤„ç†OOVï¼‰
        pred_tokens = []
        for idx in predictions[0]:
            idx_val = idx.item()
            if idx_val < len(vocab):
                token = vocab.idx2word.get(idx_val, Vocab.UNK_TOKEN)
            else:
                # OOV token
                oov_idx = idx_val - len(vocab)
                if oov_idx < len(oov_lists[0]):
                    token = oov_lists[0][oov_idx]
                else:
                    token = Vocab.UNK_TOKEN

            if token not in [Vocab.PAD_TOKEN, Vocab.SOS_TOKEN, Vocab.EOS_TOKEN]:
                pred_tokens.append(token)

        logger.info(f"ç”Ÿæˆçš„æ‘˜è¦æ ·ä¾‹: {' '.join(pred_tokens[:20])}...")

    logger.info("\n" + "=" * 50)
    logger.info("âœ… æµ‹è¯•å®Œæˆï¼Pointer-Generatoræ¨¡å‹è¿è¡Œæ­£å¸¸ï¼")
    logger.info("=" * 50)


# =======================================================
# ğŸš€ å…³é”®ä¿®æ”¹ 2: æ–°å¢ quick_test_pg_coverage å‡½æ•°
# =======================================================
def quick_test_pg_coverage(args):
    """å¿«é€Ÿæµ‹è¯•Pointer-Generator with Coverageæ¨¡å‹"""
    logger = setup_logger()
    logger.info("=" * 50)
    logger.info("å¿«é€Ÿæµ‹è¯• Pointer-Generator with Coverage æ¨¡å‹")
    logger.info("=" * 50)

    # è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")

    # 1. å‡†å¤‡æ•°æ®
    logger.info("\n[1/5] å‡†å¤‡æ•°æ®...")
    data_dir = Path(args.data_dir)
    processed_dir = data_dir.parent / 'processed'

    vocab_path = processed_dir / 'vocab.json'
    if not vocab_path.exists():
        logger.info("è¯è¡¨ä¸å­˜åœ¨ï¼Œå¼€å§‹é¢„å¤„ç†æ•°æ®...")
        from datamodules.cnndm import prepare_datasets
        # =======================================================
        # ğŸ› ä¿®å¤ prepare_datasets è°ƒç”¨å‚æ•°
        # =======================================================
        prepare_datasets(
            raw_dir=str(data_dir),
            processed_dir=str(processed_dir), 
            vocab_path=str(vocab_path),      
            max_vocab_size=args.max_vocab_size,
            min_freq=args.min_freq,
            max_src_len=args.max_src_len,
            max_tgt_len=args.max_tgt_len,
            limit_per_split=100  # é™åˆ¶æ•°æ®ï¼Œå¦åˆ™æ•°æ®é‡å¤ªå¤§
        )
        # =======================================================

    # åŠ è½½è¯è¡¨
    vocab = Vocab.load(str(vocab_path))
    logger.info(f"è¯è¡¨å¤§å°: {len(vocab)}")

    # åŠ è½½æ•°æ®
    train_loader = get_dataloader(
        processed_dir=str(processed_dir),
        batch_size=args.batch_size,
        split='train',
        shuffle=True
    )

    # åªä½¿ç”¨å°‘é‡æ•°æ®
    train_subset_indices = list(range(min(args.num_samples, len(train_loader.dataset))))
    train_subset = Subset(train_loader.dataset, train_subset_indices)
    train_loader_small = DataLoader(
        train_subset,
        batch_size=args.batch_size,
        shuffle=True,
        collate_fn=train_loader.collate_fn
    )

    logger.info(f"è®­ç»ƒæ ·æœ¬æ•°: {len(train_subset)}")

    # 2. åˆ›å»ºPointer-Generator with Coverageæ¨¡å‹
    logger.info("\n[2/5] åˆ›å»ºPointer-Generator with Coverage æ¨¡å‹...")
    # ******* [å…³é”®ä¿®æ”¹ 1: æ¨¡å‹ç±»åˆ‡æ¢] *******
    model = PGCoverageSeq2Seq(
        vocab_size=len(vocab),
        embed_size=args.embed_size,
        hidden_size=args.hidden_size,
        num_layers=args.num_layers,
        dropout=args.dropout,
        pad_idx=vocab.word2idx['<PAD>'],
        cov_loss_weight=1.0  # å…³é”®è¡¥å……ï¼šä¼ é€’è¦†ç›–æŸå¤±æƒé‡
    ).to(device)

    logger.info(f"æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")

    # 3. ä¼˜åŒ–å™¨
    logger.info("\n[3/5] åˆ›å»ºä¼˜åŒ–å™¨...")
    optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)

    # 4. è®­ç»ƒå‡ ä¸ªepoch
    logger.info(f"\n[4/5] å¼€å§‹è®­ç»ƒ {args.num_epochs} ä¸ªepoch...")

    for epoch in range(args.num_epochs):
        model.train()
        total_nll_loss = 0
        total_cov_loss = 0

        pbar = tqdm(train_loader_small, desc=f"Epoch {epoch+1}/{args.num_epochs}")
        for batch in pbar:
            src = batch['src'].to(device)
            tgt = batch['tgt'].to(device)
            src_lens = batch.get('src_lens', None)
            src_oov_map = batch.get('src_oov_map', None)

            optimizer.zero_grad()

            # ******* [å…³é”®ä¿®æ”¹ 2: æ¥æ”¶ 4 ä¸ªè¾“å‡º] *******
            output, _, _, raw_coverage_loss = model(
                src=src,
                tgt=tgt,
                src_lens=src_lens,
                src_oov_map=src_oov_map,
                teacher_forcing_ratio=1.0 # å¿«é€Ÿæµ‹è¯•ä¸­ä½¿ç”¨ 1.0 ç®€åŒ–
            )

            # è®¡ç®— NLL æŸå¤±ï¼ˆä¸ PG æ¨¡å‹é€»è¾‘ç›¸åŒï¼‰
            batch_size, seq_len, vocab_size = output.shape
            output_flat = output.reshape(-1, vocab_size)
            tgt_flat = tgt[:, 1:].reshape(-1)

            log_probs = torch.log(output_flat + 1e-10)
            target_log_probs = log_probs.gather(1, tgt_flat.unsqueeze(1)).squeeze(1)

            mask = (tgt_flat != vocab.word2idx['<PAD>']).float()
            nll_loss = -(target_log_probs * mask).sum() / mask.sum()

            # ******* [å…³é”®ä¿®æ”¹ 3: è®¡ç®— Total Loss] *******
            # åœ¨å¿«é€Ÿæµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬å‡è®¾ coverage_loss_weight (lambda) = 1.0
            total_loss = nll_loss + 1.0 * raw_coverage_loss

            # åå‘ä¼ æ’­
            total_loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)
            optimizer.step()

            total_nll_loss += nll_loss.item()
            total_cov_loss += raw_coverage_loss.item()

            # ******* [å…³é”®ä¿®æ”¹ 4: è¿›åº¦æ¡æ˜¾ç¤º] *******
            pbar.set_postfix({
                'nll': f'{nll_loss.item():.4f}',
                'cov': f'{raw_coverage_loss.item():.4f}',
                'total': f'{total_loss.item():.4f}'
            })

        avg_nll_loss = total_nll_loss / len(train_loader_small)
        avg_cov_loss = total_cov_loss / len(train_loader_small)
        avg_total_loss = avg_nll_loss + avg_cov_loss
        logger.info(f"Epoch {epoch+1} è®­ç»ƒæŸå¤±: Total={avg_total_loss:.4f} (NLL={avg_nll_loss:.4f}, Cov={avg_cov_loss:.4f})")

    # 5. æµ‹è¯•æ¨ç†ï¼ˆä¸PGæ¨¡å‹é€»è¾‘ç›¸åŒï¼‰
    logger.info("\n[5/5] æµ‹è¯•æ¨ç†åŠŸèƒ½...")
    model.eval()
    with torch.no_grad():
        test_batch = next(iter(train_loader_small))
        src = test_batch['src'][:1].to(device)
        src_lens = test_batch.get('src_lens', None)
        if src_lens is not None:
            src_lens = src_lens[:1]
        src_oov_map = test_batch.get('src_oov_map', None)
        if src_oov_map is not None:
             src_oov_map = src_oov_map[:1].to(device)

        oov_lists = test_batch.get('oov_list', [[]])[:1]

        # Greedy decoding
        # PGCoverageSeq2Seq çš„ generate æ–¹æ³•åº”è‡ªåŠ¨å¤„ç† Coverage å‘é‡
        predictions, _ = model.generate(
            src=src,
            src_lens=src_lens,
            src_oov_map=src_oov_map,
            max_length=50,
            sos_idx=vocab.word2idx['<SOS>'],
            eos_idx=vocab.word2idx['<EOS>'],
        )

        # è½¬æ¢ä¸ºæ–‡æœ¬ï¼ˆå¤„ç†OOVï¼‰
        pred_tokens = []
        for idx in predictions[0]:
            idx_val = idx.item()
            if idx_val < len(vocab):
                token = vocab.idx2word.get(idx_val, Vocab.UNK_TOKEN)
            else:
                oov_idx = idx_val - len(vocab)
                if oov_idx < len(oov_lists[0]):
                    token = oov_lists[0][oov_idx]
                else:
                    token = Vocab.UNK_TOKEN

            if token not in [Vocab.PAD_TOKEN, Vocab.SOS_TOKEN, Vocab.EOS_TOKEN]:
                pred_tokens.append(token)

        logger.info(f"ç”Ÿæˆçš„æ‘˜è¦æ ·ä¾‹: {' '.join(pred_tokens[:20])}...")

    logger.info("\n" + "=" * 50)
    logger.info("âœ… æµ‹è¯•å®Œæˆï¼Pointer-Generator with Coverage æ¨¡å‹è¿è¡Œæ­£å¸¸ï¼")
    logger.info("=" * 50)


def main():
    parser = argparse.ArgumentParser(description='å¿«é€Ÿæµ‹è¯•è®­ç»ƒè„šæœ¬')

    # æ¨¡å‹é€‰æ‹©
    # ğŸš€ å…³é”®ä¿®æ”¹ 3: å¢åŠ  'pg_cov' é€‰é¡¹
    parser.add_argument('--model', type=str, default='baseline',
                       choices=['baseline', 'pg', 'pg_cov'],
                       help='é€‰æ‹©æ¨¡å‹: baseline, pg (pointer-generator) æˆ– pg_cov (PG with Coverage)')

    # æ•°æ®å‚æ•°
    # â­â­â­ å…³é”®ä¿®å¤: å°†é»˜è®¤å€¼ä» '../data/raw' æ›´æ”¹ä¸º './data/raw' â­â­â­
    # parser.add_argument('--data_dir', type=str, default='../data/raw',
    #                    help='åŸå§‹æ•°æ®ç›®å½•')
    parser.add_argument('--data_dir', type=str, default=str(default_data_dir),
                    help='åŸå§‹æ•°æ®ç›®å½• (è‡ªåŠ¨å®šä½)')
    parser.add_argument('--num_samples', type=int, default=100,
                       help='ä½¿ç”¨çš„è®­ç»ƒæ ·æœ¬æ•°é‡ï¼ˆé»˜è®¤100ï¼‰')
    parser.add_argument('--max_vocab_size', type=int, default=50000)
    parser.add_argument('--min_freq', type=int, default=5)
    parser.add_argument('--max_src_len', type=int, default=400)
    parser.add_argument('--max_tgt_len', type=int, default=100)

    # æ¨¡å‹å‚æ•°
    parser.add_argument('--embed_size', type=int, default=256)
    parser.add_argument('--hidden_size', type=int, default=256)
    parser.add_argument('--num_layers', type=int, default=1)
    parser.add_argument('--dropout', type=float, default=0.1)
    parser.add_argument('--max_oov_size', type=int, default=1000,
                       help='Pointer-Generatoræœ€å¤§OOVè¯æ±‡æ•°')

    # è®­ç»ƒå‚æ•°
    parser.add_argument('--batch_size', type=int, default=4)
    parser.add_argument('--num_epochs', type=int, default=2,
                       help='è®­ç»ƒè½®æ•°ï¼ˆé»˜è®¤2ï¼Œåªä¸ºéªŒè¯ï¼‰')
    parser.add_argument('--learning_rate', type=float, default=1e-4)

    args = parser.parse_args()

    # æ ¹æ®æ¨¡å‹ç±»å‹è°ƒç”¨ç›¸åº”çš„æµ‹è¯•å‡½æ•°
    # ğŸš€ å…³é”®ä¿®æ”¹ 4: å¢åŠ  'pg_cov' è°ƒç”¨é€»è¾‘
    if args.model == 'baseline':
        quick_test_baseline(args)
    elif args.model == 'pg':
        quick_test_pointer_generator(args)
    elif args.model == 'pg_cov':
        quick_test_pg_coverage(args)
    else:
        # Should not happen due to argparse choices
        pass


if __name__ == '__main__':
    main()
