"""PG-Coverage æ¨¡å‹æ¨ç†è„šæœ¬ï¼šä»æ–‡æœ¬æ–‡ä»¶ç”Ÿæˆæ‘˜è¦ï¼ˆæ”¯æŒ OOV å’Œè¦†ç›–æœºåˆ¶ï¼‰."""
from __future__ import annotations

import argparse
import os
import sys
from pathlib import Path
from typing import List

import torch
import yaml

# æ·»åŠ  src åˆ° pathï¼ˆç¡®ä¿èƒ½å¯¼å…¥æ¨¡å‹å’Œå·¥å…·ç±»ï¼‰
sys.path.insert(0, str(Path(__file__).parent))

# å¯¼å…¥ PG-Coverage æ¨¡å‹ï¼ˆæ›¿æ¢ baseline æ¨¡å‹ï¼‰
from models.pointer_generator_coverage.pg_coverage_model import PGCoverageSeq2Seq
# å¯¼å…¥æ•°æ®é¢„å¤„ç†å’Œè¯è¡¨å·¥å…·
from datamodules.cnndm import prepare_datasets
from utils.vocab import Vocab  # ç¡®ä¿ Vocab ç±»æœ‰ encode/decode æ–¹æ³•

try:
    import nltk
    from nltk.tokenize import word_tokenize
    nltk.download('punkt', quiet=True)  # è‡ªåŠ¨ä¸‹è½½åˆ†è¯æ‰€éœ€èµ„æº
except ImportError as exc:
    raise ImportError("è¯·å…ˆå®‰è£… nltk (pip install nltk)") from exc


def tokenize(text: str, lowercase: bool = True) -> List[str]:
    """æ–‡æœ¬åˆ†è¯ï¼ˆä¸è®­ç»ƒæ—¶é¢„å¤„ç†é€»è¾‘ä¸€è‡´ï¼‰"""
    if lowercase:
        text = text.lower()
    return word_tokenize(text)


def load_pg_coverage_model(
    checkpoint_path: str, 
    vocab_size: int, 
    pad_idx: int, 
    device: torch.device
) -> PGCoverageSeq2Seq:
    """åŠ è½½ PG-Coverage æ¨¡å‹ï¼ˆé€‚é…æ¨¡å‹åˆå§‹åŒ–å‚æ•°ï¼‰"""
    checkpoint = torch.load(checkpoint_path, map_location=device)
    config = checkpoint.get('config', {})  # ä» checkpoint è¯»å–è®­ç»ƒæ—¶çš„é…ç½®
    
    # åˆå§‹åŒ– PG-Coverage æ¨¡å‹ï¼ˆåŒ¹é… __init__ æ–¹æ³•å‚æ•°ï¼‰
    model = PGCoverageSeq2Seq(
        vocab_size=vocab_size,
        embed_size=config.get('embed_size', 256),  # é»˜è®¤ä¸è®­ç»ƒä¸€è‡´
        hidden_size=config.get('hidden_size', 256),
        num_layers=config.get('num_layers', 1),
        dropout=config.get('dropout', 0.1),
        pad_idx=pad_idx,
        cov_loss_weight=config.get('coverage_loss_weight', 1.0)  # æ¨ç†æ—¶ä¸å½±å“ï¼Œä»…ä¸ºåˆå§‹åŒ–
    ).to(device)
    
    # åŠ è½½æ¨¡å‹æƒé‡
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()  # æ¨ç†æ¨¡å¼ï¼ˆå…³é—­ dropoutï¼‰
    return model


def summarize_text(
    model: PGCoverageSeq2Seq,
    vocab: Vocab,
    article: str,
    device: torch.device,
    max_src_len: int = 400,
    max_tgt_len: int = 100,
    decode_strategy: str = 'greedy',
    beam_size: int = 5
) -> List[str]:
    """ç”Ÿæˆæ‘˜è¦ï¼ˆè°ƒç”¨ PG-Coverage æ¨¡å‹è‡ªå¸¦çš„è§£ç æ–¹æ³•ï¼Œé€‚é…è¦†ç›–å‘é‡ï¼‰"""
    # 1. æ–‡æœ¬é¢„å¤„ç†ï¼ˆä¸è®­ç»ƒä¸€è‡´ï¼šåˆ†è¯ã€æˆªæ–­ã€ç¼–ç ï¼‰
    tokens = tokenize(article)[:max_src_len]  # æˆªæ–­è¿‡é•¿çš„æºæ–‡æœ¬
    src_indices = vocab.encode(tokens, max_len=max_src_len)  # è½¬ä¸ºè¯è¡¨ç´¢å¼•ï¼ˆè¡¥é›¶/æˆªæ–­ï¼‰
    src_tensor = torch.LongTensor([src_indices]).to(device)  # [1, max_src_len]ï¼ˆbatch_size=1ï¼‰
    src_len = torch.LongTensor([min(len(tokens), max_src_len)]).to(device)  # å®é™…æ–‡æœ¬é•¿åº¦ï¼ˆç”¨äºæ©ç ï¼‰
    
    # 2. è§£ç ï¼ˆè°ƒç”¨æ¨¡å‹è‡ªå¸¦çš„ generate/beam_search æ–¹æ³•ï¼Œè‡ªåŠ¨å¤„ç†è¦†ç›–å‘é‡ï¼‰
    if decode_strategy == 'beam':
        # Beam Search è§£ç ï¼ˆæ¨¡å‹è‡ªå¸¦ï¼Œæ”¯æŒè¦†ç›–æœºåˆ¶ï¼‰
        pred_ids, _ = model.beam_search(
            src=src_tensor,
            src_lens=src_len,
            src_oov_map=None,  # æ¨ç†æ—¶æ—  OOV æ˜ å°„ï¼ˆè‹¥è¾“å…¥æœ‰ OOVï¼Œéœ€è¡¥å……ï¼Œæ­¤å¤„ç®€åŒ–ï¼‰
            beam_size=beam_size,
            max_length=max_tgt_len,
            sos_idx=vocab.sos_idx,
            eos_idx=vocab.eos_idx,
            device=device
        )
        pred_ids = pred_ids[0].tolist()  # å–æœ€ä½³åºåˆ—ï¼ˆbatch_size=1ï¼‰
    else:
        # Greedy è§£ç ï¼ˆæ¨¡å‹è‡ªå¸¦ï¼Œè‡ªåŠ¨æ›´æ–°è¦†ç›–å‘é‡ï¼‰
        pred_ids, _ = model.generate(
            src=src_tensor,
            src_lens=src_len,
            src_oov_map=None,
            max_length=max_tgt_len,
            sos_idx=vocab.sos_idx,
            eos_idx=vocab.eos_idx,
            device=device
        )
        pred_ids = pred_ids[0].tolist()  # å–ç¬¬ä¸€ä¸ªæ ·æœ¬çš„åºåˆ—
    
    # 3. è§£ç ä¸ºæ–‡æœ¬ï¼ˆè·³è¿‡ç‰¹æ®Šç¬¦å·ï¼š<PAD>ã€<SOS>ã€<EOS>ï¼‰
    return vocab.decode(pred_ids, skip_special=True)


def collect_inputs(input_path: str) -> List[Path]:
    """æ”¶é›†è¾“å…¥æ–‡æœ¬æ–‡ä»¶ï¼ˆæ”¯æŒå•ä¸ªæ–‡ä»¶æˆ–ç›®å½•ä¸‹çš„æ‰€æœ‰ .txt æ–‡ä»¶ï¼‰"""
    path = Path(input_path)
    if path.is_dir():
        return sorted([p for p in path.glob('*.txt') if p.is_file()])
    if path.is_file() and path.suffix == '.txt':
        return [path]
    raise FileNotFoundError(f"æœªæ‰¾åˆ°æœ‰æ•ˆè¾“å…¥ï¼š{input_path}ï¼ˆä»…æ”¯æŒ .txt æ–‡ä»¶æˆ–åŒ…å« .txt çš„ç›®å½•ï¼‰")


def main(args: argparse.Namespace) -> None:
    # 1. è®¾å¤‡åˆå§‹åŒ–ï¼ˆä¼˜å…ˆ GPUï¼‰
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"âœ… ä½¿ç”¨è®¾å¤‡: {device}")

    # 2. åŠ è½½é…ç½®ï¼ˆæ”¯æŒ YAML é…ç½®æ–‡ä»¶æˆ–å‘½ä»¤è¡Œå‚æ•°ï¼‰
    if args.config:
        with open(args.config, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
    else:
        config = {}

    # 3. å…³é”®å‚æ•°é…ç½®ï¼ˆä¼˜å…ˆçº§ï¼šå‘½ä»¤è¡Œå‚æ•° > é…ç½®æ–‡ä»¶ > é»˜è®¤å€¼ï¼‰
    raw_data_dir = args.data_dir or config.get('data_dir', './data/raw')
    vocab_path = args.vocab_path or config.get('vocab_path', './data/processed/vocab.json')
    processed_dir = config.get('processed_dir', os.path.dirname(vocab_path))
    checkpoint_path = args.checkpoint or config.get('checkpoint_path', './checkpoints/best_model.pt')
    max_src_len = args.max_src_len or config.get('max_src_len', 400)
    max_tgt_len = args.max_tgt_len or config.get('max_tgt_len', 100)
    decode_strategy = args.decode_strategy
    beam_size = args.beam_size

    # 4. åŠ è½½è¯è¡¨ï¼ˆè‹¥æœªé¢„å¤„ç†ï¼Œè‡ªåŠ¨ç”Ÿæˆï¼›è‹¥å·²å­˜åœ¨ï¼Œç›´æ¥åŠ è½½ï¼‰
    print(f"ğŸ“¥ åŠ è½½è¯è¡¨: {vocab_path}")
    vocab = prepare_datasets(
        raw_dir=raw_data_dir,
        processed_dir=processed_dir,
        vocab_path=vocab_path,
        max_src_len=max_src_len,
        max_tgt_len=max_tgt_len,
        max_vocab_size=config.get('max_vocab_size', 50000),
        min_freq=config.get('min_freq', 5)
    )
    print(f"âœ… è¯è¡¨åŠ è½½å®Œæˆï¼ˆå¤§å°ï¼š{len(vocab)}ï¼‰")

    # 5. åŠ è½½ PG-Coverage æ¨¡å‹
    print(f"ğŸ“¥ åŠ è½½æ¨¡å‹: {checkpoint_path}")
    model = load_pg_coverage_model(
        checkpoint_path=checkpoint_path,
        vocab_size=len(vocab),
        pad_idx=vocab.pad_idx,
        device=device
    )
    print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼ˆæ¨ç†æ¨¡å¼ï¼‰")

    # 6. æ”¶é›†è¾“å…¥æ–‡æœ¬
    print(f"ğŸ“¥ æ”¶é›†è¾“å…¥æ–‡ä»¶: {args.input}")
    input_files = collect_inputs(args.input)
    if not input_files:
        raise FileNotFoundError(f"æœªæ‰¾åˆ°ä»»ä½• .txt æ–‡ä»¶ï¼š{args.input}")
    print(f"âœ… å…±æ”¶é›†åˆ° {len(input_files)} ä¸ªè¾“å…¥æ–‡ä»¶")

    # 7. åˆ›å»ºè¾“å‡ºç›®å½•ï¼ˆç¡®ä¿è¾“å‡ºè·¯å¾„å­˜åœ¨ï¼‰
    output_path = Path(args.output)
    os.makedirs(output_path.parent, exist_ok=True)

    # 8. æ‰¹é‡æ¨ç†å¹¶ä¿å­˜ç»“æœ
    results = []
    print(f"\nğŸš€ å¼€å§‹ç”Ÿæˆæ‘˜è¦ï¼ˆç­–ç•¥ï¼š{decode_strategy}ï¼Œæœ€å¤§é•¿åº¦ï¼š{max_tgt_len}ï¼‰")
    for idx, filepath in enumerate(input_files, start=1):
        # è¯»å–è¾“å…¥æ–‡æœ¬
        with filepath.open('r', encoding='utf-8') as f:
            article = f.read().strip()
        if not article:
            print(f"âš ï¸ è·³è¿‡ç©ºæ–‡ä»¶ï¼š{filepath.name}")
            continue

        # ç”Ÿæˆæ‘˜è¦ï¼ˆæ— æ¢¯åº¦è®¡ç®—ï¼ŒåŠ é€Ÿï¼‰
        with torch.no_grad():
            summary_tokens = summarize_text(
                model=model,
                vocab=vocab,
                article=article,
                device=device,
                max_src_len=max_src_len,
                max_tgt_len=max_tgt_len,
                decode_strategy=decode_strategy,
                beam_size=beam_size
            )
        summary = ' '.join(summary_tokens)  # è½¬ä¸ºå­—ç¬¦ä¸²

        # ä¿å­˜ç»“æœ
        results.append({
            'id': idx,
            'file': str(filepath),
            'article_length': len(article),
            'summary': summary
        })

        # æ‰“å°è¿›åº¦
        print(f"\n--- æ ·æœ¬ {idx}ï¼ˆ{filepath.name}ï¼‰---")
        print(f"æºæ–‡æœ¬ï¼ˆå‰100å­—ç¬¦ï¼‰: {article[:100]}...")
        print(f"ç”Ÿæˆæ‘˜è¦: {summary}")

    # 9. ä¿å­˜æ‰€æœ‰ç»“æœåˆ°æ–‡ä»¶
    with output_path.open('w', encoding='utf-8') as f:
        for item in results:
            f.write(f"=== SAMPLE {item['id']} ===\n")
            f.write(f"File: {item['file']}\n")
            f.write(f"Article Length: {item['article_length']} characters\n")
            f.write(f"Summary: {item['summary']}\n\n")

    print(f"\nğŸ‰ æ¨ç†å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
    print(f"ğŸ“Š ç»Ÿè®¡ï¼šå…±å¤„ç† {len(results)} ä¸ªæœ‰æ•ˆæ–‡ä»¶ï¼Œç”Ÿæˆ {len(results)} æ¡æ‘˜è¦")


if __name__ == '__main__':
    # å‘½ä»¤è¡Œå‚æ•°è§£æ
    parser = argparse.ArgumentParser(description='PG-Coverage æ¨¡å‹æ¨ç†è„šæœ¬ï¼ˆæ”¯æŒ OOV å’Œè¦†ç›–æœºåˆ¶ï¼‰')
    # é…ç½®ä¸æ¨¡å‹
    parser.add_argument('--config', type=str, help='YAML é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰')
    parser.add_argument('--checkpoint', type=str, help='PG-Coverage æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„ï¼ˆå¿…å¡«æˆ–åœ¨é…ç½®æ–‡ä»¶ä¸­æŒ‡å®šï¼‰')
    parser.add_argument('--vocab_path', type=str, help='è¯è¡¨è·¯å¾„ï¼ˆå¿…å¡«æˆ–åœ¨é…ç½®æ–‡ä»¶ä¸­æŒ‡å®šï¼‰')
    parser.add_argument('--data_dir', type=str, help='åŸå§‹æ•°æ®ç›®å½•ï¼ˆç”¨äºç”Ÿæˆè¯è¡¨ï¼Œå¯é€‰ï¼‰')
    # è¾“å…¥è¾“å‡º
    parser.add_argument('--input', type=str, required=True, help='è¾“å…¥ï¼š.txt æ–‡ä»¶è·¯å¾„æˆ–åŒ…å« .txt çš„ç›®å½•ï¼ˆå¿…å¡«ï¼‰')
    parser.add_argument('--output', type=str, default='../docs/pg_coverage_infer_results.txt', help='è¾“å‡ºç»“æœæ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤ï¼š../docs/pg_coverage_infer_results.txtï¼‰')
    # æ¨ç†å‚æ•°
    parser.add_argument('--max_src_len', type=int, help='æºæ–‡æœ¬æœ€å¤§é•¿åº¦ï¼ˆé»˜è®¤ï¼š400ï¼‰')
    parser.add_argument('--max_tgt_len', type=int, help='æ‘˜è¦æœ€å¤§é•¿åº¦ï¼ˆé»˜è®¤ï¼š100ï¼‰')
    parser.add_argument('--decode_strategy', type=str, default='greedy', choices=['greedy', 'beam'], help='è§£ç ç­–ç•¥ï¼ˆé»˜è®¤ï¼šgreedyï¼‰')
    parser.add_argument('--beam_size', type=int, default=5, help='Beam Search å¤§å°ï¼ˆä»… beam ç­–ç•¥éœ€è¦ï¼Œé»˜è®¤ï¼š5ï¼‰')

    args = parser.parse_args()
    main(args)
