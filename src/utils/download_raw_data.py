"""
ç‹¬ç«‹çš„æ•°æ®ä¸‹è½½ä¸åŸå§‹æ•°æ®æ ¼å¼è½¬æ¢è„šæœ¬ã€‚

ç”¨äºåœ¨è¿è¡Œ quick_test_train.py æˆ– train_*.py ä¹‹å‰ï¼Œ
ç¡®ä¿ raw data/raw/cnn_dailymail å­˜åœ¨ï¼Œä»è€Œé¿å… prepare_datasets æŠ¥é”™ã€‚
"""
from __future__ import annotations

import os
import sys
from pathlib import Path
from typing import List
from tqdm import tqdm

# ç¡®ä¿èƒ½å¯¼å…¥é¡¹ç›®è·¯å¾„ä¸­çš„ä¾èµ–
sys.path.insert(0, str(Path(__file__).parent.parent))

def ensure_raw_dataset(raw_dir: str, dataset_version: str = '3.0.0') -> None:
    """ä¿è¯åŸå§‹ CNN/DailyMail æ–‡æœ¬å­˜åœ¨ï¼Œç¼ºå¤±æ—¶å°è¯•è‡ªåŠ¨ä¸‹è½½ã€‚"""
    raw_dir_path = Path(raw_dir)
    raw_dir_path.mkdir(parents=True, exist_ok=True)

    expected_splits = ['train', 'validation', 'test']
    missing_splits = []

    # æ£€æŸ¥åŸå§‹æ•°æ®ç›®å½•æ˜¯å¦ä¸ºç©ºæˆ–ç¼ºå¤±
    for split in expected_splits:
        split_path = raw_dir_path / split

        # åªæœ‰åœ¨ç›®å½•ä¸‹æ²¡æœ‰ .txt æ–‡ä»¶æ—¶æ‰ç®—ç¼ºå¤±
        if not split_path.is_dir() or not any(split_path.glob('*.txt')):
            missing_splits.append(split)

    if not missing_splits:
        print("âœ“ åŸå§‹æ•°æ®ç›®å½•å·²å­˜åœ¨ä¸”åŒ…å«æ–‡ä»¶ï¼Œè·³è¿‡ä¸‹è½½ã€‚")
        return

    print(
        "ğŸš¨ æ£€æµ‹åˆ°åŸå§‹æ•°æ®ç¼ºå¤±: "
        f"{missing_splits} åˆ’åˆ†ï¼Œå¼€å§‹ä» Hugging Face ä¸‹è½½ CNN/DailyMail ({dataset_version})..."
    )

    try:
        from datasets import load_dataset  # type: ignore
    except ImportError as exc:
        raise RuntimeError(
            "ğŸ›‘ ç¼ºå°‘ CNN/DailyMail åŸå§‹æ•°æ®ï¼Œä¸”æœªå®‰è£… `datasets` åº“ï¼Œæ— æ³•è‡ªåŠ¨ä¸‹è½½ã€‚\n"
            "è¯·æ‰§è¡Œ `pip install datasets` æˆ–æ‰‹åŠ¨å°†æ•°æ®æ”¾ç½®åœ¨ data/raw ç›®å½•ä¸‹ã€‚"
        ) from exc

    # ç»Ÿä¸€ä½¿ç”¨ 'cnn_dailymail'
    dataset = load_dataset('cnn_dailymail', dataset_version)
    print("ä¸‹è½½å®Œæˆï¼Œå¼€å§‹å¯¼å‡ºä¸ºé¡¹ç›®æ‰€éœ€æ ¼å¼...")

    for split in missing_splits:
        # HuggingFace split name: 'validation' for 'val', others match
        hf_split = 'validation' if split == 'validation' else split
        subset = dataset[hf_split]
        split_path = raw_dir_path / split
        split_path.mkdir(parents=True, exist_ok=True)
        print(f"å¯¼å‡º {split} åˆ’åˆ†ï¼Œå…± {len(subset)} ä¸ªæ ·æœ¬...")

        # tqdm åŒ…è£…è¿­ä»£å™¨ä»¥æ˜¾ç¤ºè¿›åº¦æ¡
        for idx, example in enumerate(tqdm(subset, desc=f"Writing {split}", unit='sample')):
            filename = f"{split}_{idx:06d}.txt"
            filepath = split_path / filename
            article = example['article'].strip()
            # CNN/DailyMail ä½¿ç”¨ 'highlights' å­—æ®µä½œä¸ºæ‘˜è¦
            summary = example['highlights'].strip()

            with filepath.open('w', encoding='utf-8') as f:
                f.write("=== ARTICLE ===\n")
                f.write(article)
                f.write("\n\n=== SUMMARY ===\n")
                f.write(summary)

        print(f"âœ“ {split} åˆ’åˆ†å¯¼å‡ºå®Œæˆ: {split_path}")

    print("ğŸ‰ å·²å®Œæˆ CNN/DailyMail æ•°æ®é›†ä¸‹è½½ä¸å¯¼å‡ºã€‚")

def main():
    # å‡è®¾é¡¹ç›®ç»“æ„æ˜¯ AI27_summarization/src/utils/download_raw_data.py
    project_root = Path(__file__).parent.parent.parent.resolve()
    raw_data_dir = str(project_root / 'data' / 'raw')

    # ç¡®ä¿ datasets åº“å·²å®‰è£…
    try:
        import datasets
    except ImportError:
        print("ğŸš¨ æ­£åœ¨å®‰è£… datasets åº“...")
        os.system(f"{sys.executable} -m pip install datasets")
        import datasets

    # æ£€æŸ¥æ˜¯å¦åœ¨æ­£ç¡®çš„ç›®å½•ï¼Œå¹¶åˆ‡æ¢
    if Path.cwd() != project_root:
        print(f"åˆ‡æ¢åˆ°é¡¹ç›®æ ¹ç›®å½•: {project_root}")
        os.chdir(project_root)

    ensure_raw_dataset(raw_data_dir, dataset_version='3.0.0')

if __name__ == '__main__':
    main()
