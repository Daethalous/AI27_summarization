"""
Transformer+Pointer-Generator+Coverage (PGCT_layer) æ¨¡å‹æ­£å¼è®­ç»ƒè„šæœ¬
æ”¯æŒå®šæœŸä¿å­˜ checkpointï¼Œæ–°å¢é…ç½®æ–‡ä»¶å‚æ•°åŠ è½½ï¼ˆä¼˜å…ˆçº§ï¼šå‘½ä»¤è¡Œ>é…ç½®æ–‡ä»¶>é»˜è®¤å€¼ï¼‰
"""
from __future__ import annotations
import sys
from pathlib import Path
import logging
from typing import Optional, List
import argparse
import yaml

import torch
import torch.nn as nn  # [NEW] å¼•å…¥ nn
from torch.utils.data import DataLoader, Subset
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm
# [MODIFIED] ç§»é™¤ torch.optim as optimï¼Œæ”¹ç”¨ AdamW
from torch.optim import AdamW
# -------------------------------------------------------------------------
# [MODIFIED] å¼•å…¥å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼šLambdaLR ç”¨äºè‡ªå®šä¹‰ Warmup+Cosine
from torch.optim.lr_scheduler import LambdaLR

# [MODIFIED] ç§»é™¤ from torch.optim.lr_scheduler import ReduceLROnPlateau
# -------------------------------------------------------------------------

sys.path.insert(0, str(Path(__file__).parent.parent))

from datamodules.cnndm import prepare_datasets, get_dataloader
from utils.vocab import Vocab
# -------------------------------------------------------------------------
from models.pgct_layer.pgct_layer_model import PGCT_layer_Model
from models.pgct_layer.pgct_decoding import pgct_greedy_decode

# -------------------------------------------------------------------------

try:
    from utils.metrics import compute_rouge

    HAS_ROUGE = True
except ImportError:
    HAS_ROUGE = False


def setup_logger():
    logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
    return logging.getLogger(__name__)


# [NEW] Warmup + Cosine Annealing è°ƒåº¦å™¨å‡½æ•°
def get_optimizer_and_scheduler(model: nn.Module, args: argparse.Namespace, total_steps: int):
    """
    åˆå§‹åŒ– AdamW ä¼˜åŒ–å™¨å’Œ Warmup + Cosine Annealing å­¦ä¹ ç‡è°ƒåº¦å™¨ã€‚
    """
    base_lr = args.learning_rate
    weight_decay = args.weight_decay
    warmup_steps = args.warmup_steps

    # 1. åˆå§‹åŒ–ä¼˜åŒ–å™¨ (AdamW)
    optimizer = AdamW(
        model.parameters(),
        lr=base_lr,
        weight_decay=weight_decay
    )

    # 2. å®šä¹‰å­¦ä¹ ç‡è°ƒåº¦å‡½æ•° (lr_lambda)
    def lr_lambda(step):
        step = max(step, 0)

        # a. Warmup é˜¶æ®µ: çº¿æ€§å¢åŠ 
        if step < warmup_steps:
            # å­¦ä¹ ç‡ä» 0 çº¿æ€§å¢åŠ åˆ° 1.0 (å³ base_lr)
            return float(step) / float(warmup_steps)

        # b. Cosine Annealing é˜¶æ®µ: ä½™å¼¦é€€ç«è¡°å‡
        decay_steps = total_steps - warmup_steps
        current_decay_step = step - warmup_steps

        # å¦‚æœè®­ç»ƒå·²ç»è¶…è¿‡æ€»æ­¥æ•°æˆ–è¡°å‡æ­¥æ•°ä¸åˆç†
        if current_decay_step >= decay_steps or decay_steps <= 0:
            return 0.0

        # è®¡ç®—è¡°å‡è¿›åº¦ (0.0 åˆ° 1.0)
        progress = float(current_decay_step) / float(decay_steps)

        # ä½™å¼¦è¡°å‡å…¬å¼: 0.5 * (1 + cos(pi * progress))
        return 0.5 * (1.0 + torch.cos(torch.tensor(torch.pi * progress)))

    # 3. åˆå§‹åŒ–è°ƒåº¦å™¨
    # LambdaLR ä½¿ç”¨ lr_lambda å‡½æ•°æ¥è®¡ç®—ä¹˜æ•°å› å­ï¼Œä¹˜ä»¥ base_lr å¾—åˆ°å®é™…å­¦ä¹ ç‡
    scheduler = LambdaLR(optimizer, lr_lambda)

    return optimizer, scheduler


def calculate_nll_loss(predictions: torch.Tensor, targets: torch.Tensor, pad_idx: int) -> torch.Tensor:
    """
    è®¡ç®—è´Ÿå¯¹æ•°ä¼¼ç„¶æŸå¤± (NLL)ã€‚
    æ­¤å‡½æ•°ç°åœ¨å¿…é¡»èƒ½å¤Ÿå¤„ç† targets ä¸­çš„æ‰©å±•è¯è¡¨ç´¢å¼• (OOV è¯)ã€‚
    """
    # predictions: [B, T_out, V_ext] (V_ext = vocab_size + max_oov_size)
    # targets: [B, T_out] (T_out = T - 1, å› ä¸ºç›®æ ‡åºåˆ—ç§»ä½äº†ï¼Œtargets åŒ…å«æ‰©å±•ç´¢å¼•)
    B, T, V = predictions.shape
    preds_flat = predictions.reshape(-1, V)
    targs_flat = targets.reshape(-1)

    # ä½¿ç”¨ log(P) ä»¥ç¡®ä¿æ•°å€¼ç¨³å®šæ€§
    log_probs = torch.log(preds_flat + 1e-12)  # é¿å… log(0)

    # é’ˆå¯¹ç›®æ ‡ç´¢å¼• targs_flat æ”¶é›†å¯¹åº”çš„ log æ¦‚ç‡
    # targs_flat çš„å€¼å¯ä»¥å¤§äº vocab_size (å¯¹åº” OOV è¯)
    picked = log_probs.gather(1, targs_flat.unsqueeze(1)).squeeze(1)

    # è®¡ç®—æœ‰æ•ˆè¯çš„æ©ç  (é PAD è¯)
    mask = (targs_flat != pad_idx).float()

    # NLL æŸå¤±: -log(P) çš„å¹³å‡å€¼
    loss = -(picked * mask).sum() / mask.sum()
    return loss


def generate_val_summaries(model, val_loader, vocab, device, max_tgt_len):
    """ç”ŸæˆéªŒè¯é›†æ‘˜è¦ï¼ˆç”¨äºè®¡ç®— ROUGEï¼‰"""
    model.eval()
    generated_summaries = []
    reference_summaries = []

    with torch.no_grad():
        for batch in tqdm(val_loader, desc="ç”ŸæˆéªŒè¯é›†æ‘˜è¦"):
            src = batch['src'].to(device)
            src_len = batch['src_len'].to(device)
            src_oov_map = batch['src_oov_map'].to(device)
            oov_dicts = batch['oov_dicts']  # æ¯ä¸ªæ ·æœ¬çš„ OOV è¯æ˜ å°„
            references = batch['tgt_text']  # å‚è€ƒæ‘˜è¦æ–‡æœ¬

            # è´ªå¿ƒè§£ç ç”Ÿæˆæ‘˜è¦
            pred_ids, _ = pgct_greedy_decode(
                model=model,
                src=src,
                src_lens=src_len,
                src_oov_map=src_oov_map,
                max_length=max_tgt_len,
                sos_idx=vocab.sos_idx,
                eos_idx=vocab.eos_idx,
                device=device
            )

            # å°†é¢„æµ‹ç´¢å¼•è½¬æ¢ä¸ºæ–‡æœ¬ï¼ˆå¤„ç† OOVï¼‰
            for i in range(len(pred_ids)):
                pred_tokens = []
                oov_dict = oov_dicts[i]  # å½“å‰æ ·æœ¬çš„ OOV æ˜ å°„
                for idx in pred_ids[i].tolist():
                    if idx < len(vocab):
                        # ä½¿ç”¨ vocab.UNK_TOKEN (å¦‚æœå­˜åœ¨)ï¼Œå¦åˆ™é»˜è®¤ä¸º '<unk>'
                        token = vocab.idx2word.get(idx, vocab.UNK_TOKEN)
                    else:
                        oov_rel_idx = idx - len(vocab)
                        token = oov_dict.get(oov_rel_idx, vocab.UNK_TOKEN)
                    if token == vocab.EOS_TOKEN:
                        break  # é‡åˆ° EOS åœæ­¢
                    if token not in [vocab.PAD_TOKEN, vocab.SOS_TOKEN]:
                        pred_tokens.append(token)
                generated_summaries.append(' '.join(pred_tokens))
                reference_summaries.append(references[i])

    return generated_summaries, reference_summaries


def main():
    parser = argparse.ArgumentParser()
    # æ–°å¢ï¼šæ·»åŠ  --config å‚æ•°ï¼Œç”¨äºæŒ‡å®šYAMLé…ç½®æ–‡ä»¶è·¯å¾„
    parser.add_argument("--config", type=str, help="YAMLé…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆä¾‹å¦‚ ../configs/pgct_layer.yamlï¼‰")
    parser.add_argument(
        "--resume_ckpt_path",
        type=str,
        default=None,
        help="ç”¨äºæ¢å¤è®­ç»ƒçš„ Checkpoint æ–‡ä»¶è·¯å¾„"
    )
    # åŸæœ‰å‚æ•°ä¿ç•™ï¼Œé»˜è®¤å€¼å°†ä½œä¸ºæœ€ä½ä¼˜å…ˆçº§
    parser.add_argument("--data_dir", type=str, default="../data/raw")
    parser.add_argument("--save_dir", type=str, default="../checkpoints_pgct_layer")
    parser.add_argument("--num_epochs", type=int, default=10)
    parser.add_argument("--batch_size", type=int, default=8)
    parser.add_argument("--embed_size", type=int, default=512)
    parser.add_argument("--hidden_size", type=int, default=512)
    parser.add_argument("--num_encoder_layers", type=int, default=3)
    parser.add_argument("--num_decoder_layers", type=int, default=3)
    parser.add_argument("--nhead", type=int, default=8)
    parser.add_argument("--dropout", type=float, default=0.1)
    parser.add_argument("--cov_loss_weight", type=float, default=1.0)
    parser.add_argument("--max_src_len", type=int, default=400)
    parser.add_argument("--max_tgt_len", type=int, default=100)
    parser.add_argument("--teacher_forcing_ratio", type=float, default=0.5)
    parser.add_argument("--learning_rate", type=float, default=1e-4)
    # [NEW] ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨å‚æ•°
    parser.add_argument("--weight_decay", type=float, default=0.01, help="AdamW æƒé‡è¡°å‡")
    parser.add_argument("--warmup_steps", type=int, default=4000, help="Warmup æ­¥æ•°")
    # -------------------------------------------------------------------------
    parser.add_argument("--grad_clip", type=float, default=5.0)
    parser.add_argument("--save_every", type=int, default=2, help="éš”å¤šå°‘ä¸ª epoch ä¿å­˜ä¸€æ¬¡ checkpoint")
    parser.add_argument("--num_samples", type=int, default=None, help="é™åˆ¶è®­ç»ƒé›†ä½¿ç”¨çš„æ ·æœ¬æ•°é‡ (Noneè¡¨ç¤ºä½¿ç”¨å…¨éƒ¨)")
    # -------------------------------------------------------------------------
    args = parser.parse_args()

    logger = setup_logger()
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")

    # -------------------------------------------------------------------------
    # æ–°å¢ï¼šåŠ è½½YAMLé…ç½®æ–‡ä»¶ï¼ˆå¦‚æœæŒ‡å®šï¼‰
    config = {}
    if args.config:
        config_path = Path(args.config)
        if not config_path.exists():
            raise FileNotFoundError(f"æŒ‡å®šçš„é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
        with open(config_path, "r", encoding="utf-8") as f:
            config = yaml.safe_load(f)
        logger.info(f"âœ…  æˆåŠŸåŠ è½½é…ç½®æ–‡ä»¶: {args.config}")
    # -------------------------------------------------------------------------

    # -------------------------------------------------------------------------
    # æ–°å¢ï¼šå‚æ•°ä¼˜å…ˆçº§å¤„ç†ï¼ˆå‘½ä»¤è¡Œ > é…ç½®æ–‡ä»¶ > è„šæœ¬é»˜è®¤å€¼ï¼‰
    data_config = config.get("data", {})
    # ... (æ•°æ®å‚æ•°åŠ è½½)
    args.data_dir = args.data_dir or data_config.get("data_dir", "../data/raw")
    args.max_src_len = args.max_src_len or data_config.get("max_src_len", 400)
    args.max_tgt_len = args.max_tgt_len or data_config.get("max_tgt_len", 100)
    # [FIX] å¼ºåˆ¶è½¬æ¢ min_freq å’Œ max_vocab_size ä¸ºæ•´æ•°ï¼Œå¹¶ä¿æŒé…ç½®æ–‡ä»¶ä¼˜å…ˆçº§

    # 1. å¤„ç† max_vocab_size
    config_mvs = data_config.get("max_vocab_size")
    if config_mvs is not None:
        try:
            # å°è¯•å°†é…ç½®æ–‡ä»¶ä¸­çš„å€¼è½¬æ¢ä¸º int
            max_vocab_size = int(config_mvs)
        except (TypeError, ValueError):
            # å¦‚æœè½¬æ¢å¤±è´¥ï¼ˆå€¼æ˜¯æ— æ•ˆå­—ç¬¦ä¸²ï¼‰ï¼Œåˆ™å›é€€åˆ°ç¡¬ç¼–ç é»˜è®¤å€¼
            logger.warning(f"é…ç½®æ–‡ä»¶ä¸­çš„ max_vocab_size: '{config_mvs}' æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤å€¼ 50000ã€‚")
            max_vocab_size = 50000
    else:
        # å¦‚æœé…ç½®æ–‡ä»¶ä¸­æœªè®¾ç½®è¯¥é”®ï¼Œåˆ™ä½¿ç”¨é»˜è®¤å€¼
        max_vocab_size = 50000

    # 2. å¤„ç† min_freq
    config_mf = data_config.get("min_freq")
    if config_mf is not None:
        try:
            # å°è¯•å°†é…ç½®æ–‡ä»¶ä¸­çš„å€¼è½¬æ¢ä¸º int
            min_freq = int(config_mf)
        except (TypeError, ValueError):
            # å¦‚æœè½¬æ¢å¤±è´¥ï¼ˆå€¼æ˜¯æ— æ•ˆå­—ç¬¦ä¸²ï¼‰ï¼Œåˆ™å›é€€åˆ°ç¡¬ç¼–ç é»˜è®¤å€¼
            logger.warning(f"é…ç½®æ–‡ä»¶ä¸­çš„ min_freq: '{config_mf}' æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤å€¼ 5ã€‚")
            min_freq = 5
    else:
        # å¦‚æœé…ç½®æ–‡ä»¶ä¸­æœªè®¾ç½®è¯¥é”®ï¼Œåˆ™ä½¿ç”¨é»˜è®¤å€¼
        min_freq = 5

    model_config = config.get("model", {})
    # ... (æ¨¡å‹å‚æ•°åŠ è½½)
    args.embed_size = args.embed_size or model_config.get("embed_size", 512)
    args.hidden_size = args.hidden_size or model_config.get("hidden_size", 512)
    args.num_encoder_layers = args.num_encoder_layers or model_config.get("num_encoder_layers", 3)
    args.num_decoder_layers = args.num_decoder_layers or model_config.get("num_decoder_layers", 3)
    args.nhead = args.nhead or model_config.get("nhead", 8)
    args.dropout = args.dropout or model_config.get("dropout", 0.1)
    args.cov_loss_weight = args.cov_loss_weight or model_config.get("cov_loss_weight", 1.0)

    train_config = config.get("train", {})
    args.save_dir = args.save_dir or train_config.get("save_dir", "../checkpoints_pgct_layer")
    args.num_epochs = args.num_epochs or train_config.get("num_epochs", 10)
    args.batch_size = args.batch_size or train_config.get("batch_size", 8)
    args.learning_rate = args.learning_rate or train_config.get("learning_rate", 1e-4)
    args.teacher_forcing_ratio = args.teacher_forcing_ratio or train_config.get("teacher_forcing_ratio", 0.5)
    args.grad_clip = args.grad_clip or train_config.get("grad_clip", 5.0)
    args.save_every = args.save_every or train_config.get("save_every", 2)
    args.num_samples = args.num_samples or train_config.get("num_samples", None)

    # [NEW] ä»é…ç½®æˆ–å‘½ä»¤è¡ŒåŠ è½½æ–°çš„ä¼˜åŒ–å™¨/è°ƒåº¦å™¨å‚æ•°
    args.weight_decay = args.weight_decay or train_config.get("weight_decay", 0.01)
    args.warmup_steps = args.warmup_steps or train_config.get("warmup_steps", 4000)
    # -------------------------------------------------------------------------

    # æ‰“å°æœ€ç»ˆç”Ÿæ•ˆçš„æ ¸å¿ƒå‚æ•°ï¼ˆæ–¹ä¾¿éªŒè¯ä¼˜å…ˆçº§ï¼‰
    logger.info(f"ğŸ”§  æœ€ç»ˆç”Ÿæ•ˆçš„æ ¸å¿ƒå‚æ•°:")
    logger.info(f"  - æ¨¡å‹å‚æ•°: hidden_size={args.hidden_size}, embed_size={args.embed_size}, nhead={args.nhead}")
    logger.info(
        f"  - è®­ç»ƒå‚æ•°: batch_size={args.batch_size}, num_epochs={args.num_epochs}, peak_lr={args.learning_rate}, warmup_steps={args.warmup_steps}, weight_decay={args.weight_decay}")  # [MODIFIED]
    logger.info(
        f"  - æ•°æ®å‚æ•°: max_src_len={args.max_src_len}, max_tgt_len={args.max_tgt_len}, max_vocab_size={max_vocab_size}")
    # -------------------------------------------------------------------------

    # æ•°æ®é¢„å¤„ç†
    # ... (æ•°æ®åŠ è½½å’Œè¯è¡¨åˆå§‹åŒ–ä¿æŒä¸å˜)
    data_dir = Path(args.data_dir)
    processed_dir = data_dir.parent / "processed"
    vocab_path = data_config.get("vocab_path", processed_dir / "vocab.json")
    vocab_path = Path(vocab_path)

    logger.warning(f"ç¡®ä¿ {processed_dir} ä¸­çš„ç¼“å­˜æ–‡ä»¶åŒ…å« PG æ‰€éœ€çš„åŸå§‹ tokensï¼Œè‹¥æ²¡æœ‰ï¼Œå°†é‡æ–°ç”Ÿæˆæ•°æ®...")
    prepare_datasets(
        str(data_dir),
        str(processed_dir),
        str(vocab_path),
        max_vocab_size=max_vocab_size,
        min_freq=min_freq,
        max_src_len=args.max_src_len,
        max_tgt_len=args.max_tgt_len
    )

    vocab = Vocab.load(str(vocab_path))
    pad_idx = vocab.pad_idx
    sos_idx = vocab.sos_idx
    eos_idx = vocab.eos_idx
    logger.info(f"è¯è¡¨å·²åŠ è½½: {len(vocab)} ä¸ªè¯")

    full_train_loader = get_dataloader(
        str(processed_dir),
        batch_size=args.batch_size,
        split="train",
        shuffle=True,
        vocab=vocab,
        include_oov=True
    )

    if args.num_samples is not None and args.num_samples < len(full_train_loader.dataset):
        indices = list(range(args.num_samples))
        subset_dataset = Subset(full_train_loader.dataset, indices)
        train_loader = DataLoader(
            subset_dataset,
            batch_size=args.batch_size,
            shuffle=True,
            collate_fn=full_train_loader.collate_fn,
            num_workers=full_train_loader.num_workers
        )
        logger.info(f"ğŸš§  é™åˆ¶è®­ç»ƒé›†å¤§å°ä¸º {args.num_samples} ä¸ªæ ·æœ¬ã€‚")
    else:
        train_loader = full_train_loader

    val_loader = get_dataloader(
        str(processed_dir),
        batch_size=args.batch_size,
        split="val",
        shuffle=False,
        vocab=vocab,
        include_oov=True
    )

    # æ¨¡å‹
    model = PGCT_layer_Model(
        vocab_size=len(vocab),
        embed_size=args.embed_size,
        hidden_size=args.hidden_size,
        num_encoder_layers=args.num_encoder_layers,
        num_decoder_layers=args.num_decoder_layers,
        nhead=args.nhead,
        dropout=args.dropout,
        pad_idx=pad_idx,
        cov_loss_weight=args.cov_loss_weight,
        max_src_len=args.max_src_len,
        max_tgt_len=args.max_tgt_len
    ).to(device)
    logger.info("PGCT_layer_Model åˆå§‹åŒ–å®Œæˆ")

    # åˆ›å»ºä¿å­˜ç›®å½•
    Path(args.save_dir).mkdir(parents=True, exist_ok=True)

    # [NEW] è®¡ç®—æ€»æ­¥æ•°
    total_steps = len(train_loader) * args.num_epochs

    # [MODIFIED] ä½¿ç”¨æ–°çš„å‡½æ•°åˆå§‹åŒ– AdamW ä¼˜åŒ–å™¨å’Œ Warmup+Cosine è°ƒåº¦å™¨
    optimizer, scheduler = get_optimizer_and_scheduler(model, args, total_steps)  # [MODIFIED]

    tb_writer = SummaryWriter(log_dir=Path(args.save_dir) / "runs")

    start_epoch = 1
    best_val_loss = float("inf")
    best_rouge_l = -float("inf")

    # [NEW/MODIFIED] Checkpoint æ¢å¤æœºåˆ¶
    if args.resume_ckpt_path and Path(args.resume_ckpt_path).exists():
        ckpt_path = Path(args.resume_ckpt_path)
        logger.info(f"ğŸ’¾  å°è¯•ä» Checkpoint æ¢å¤è®­ç»ƒ: {ckpt_path}")

        try:
            checkpoint = torch.load(ckpt_path, map_location=device)

            # æ¢å¤æ¨¡å‹å’Œä¼˜åŒ–å™¨çŠ¶æ€
            model.load_state_dict(checkpoint['model_state_dict'])
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

            # æ¢å¤è®­ç»ƒè¿›åº¦
            start_epoch = checkpoint['epoch'] + 1
            # æ¢å¤æœ€ä½³æŒ‡æ ‡
            best_val_loss = checkpoint.get('best_val_loss', checkpoint.get('avg_val_loss', float("inf")))
            best_rouge_l = checkpoint.get('best_rouge_l', -float("inf"))

            # [NEW] æ¢å¤è°ƒåº¦å™¨è¿›åº¦ (Warmup/Cosine å¿…é¡»å¿«è¿›)
            steps_per_epoch = len(train_loader)
            steps_completed = (start_epoch - 1) * steps_per_epoch

            # ä½¿ç”¨ scheduler.step() å¿«è¿›åˆ°æ­£ç¡®çš„ä½ç½®
            for _ in range(steps_completed):
                scheduler.step()

            current_lr = scheduler.get_last_lr()[0]
            logger.info(
                f"âœ…  æˆåŠŸæ¢å¤ï¼ä» Epoch {start_epoch} å¼€å§‹è®­ç»ƒã€‚å½“å‰æœ€ä½³ Val Loss: {best_val_loss:.4f}ã€‚å½“å‰ LR: {current_lr:.6e}")

        except Exception as e:
            logger.error(f"âŒ  Checkpoint åŠ è½½å¤±è´¥: {e}. å°†ä» Epoch 1 é‡æ–°å¼€å§‹ã€‚")
            start_epoch = 1

    # [NEW] åˆå§‹åŒ–å½“å‰è®­ç»ƒæ­¥æ•°ï¼Œç”¨äº Warmup/Cosine è°ƒåº¦
    current_step = (start_epoch - 1) * len(train_loader)

    # [MODIFIED] å¾ªç¯ä» start_epoch å¼€å§‹
    for epoch in range(start_epoch, args.num_epochs + 1):
        model.train()
        running_nll = 0.0
        running_cov = 0.0
        pbar = tqdm(train_loader, desc=f"Train Epoch {epoch}/{args.num_epochs}")

        for batch in pbar:
            src = batch['src'].to(device)
            tgt = batch['tgt'].to(device)
            src_oov_map = batch['src_oov_map'].to(device)

            optimizer.zero_grad()

            outputs, _, _, coverage_loss = model(
                src,
                tgt,
                src_oov_map=src_oov_map,
                teacher_forcing_ratio=args.teacher_forcing_ratio
            )

            nll_loss = calculate_nll_loss(outputs, tgt[:, 1:], pad_idx)
            total_loss = nll_loss + coverage_loss

            total_loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=args.grad_clip)

            # [MODIFIED] æ¯æ­¥æ›´æ–°ä¼˜åŒ–å™¨
            optimizer.step()

            # [NEW] æ¯æ­¥æ›´æ–°å­¦ä¹ ç‡è°ƒåº¦å™¨ (Warmup/Cosine)
            scheduler.step()
            current_step += 1  # æ›´æ–°å…¨å±€æ­¥æ•°

            running_nll += nll_loss.item()
            running_cov += coverage_loss.item()

            # [MODIFIED] ä½¿ç”¨è°ƒåº¦å™¨è·å–çš„æœ€æ–° LR
            current_lr = scheduler.get_last_lr()[0]
            pbar.set_postfix(
                {"NLL": f"{nll_loss.item():.4f}", "Cov": f"{coverage_loss.item():.4f}", "LR": f"{current_lr:.6e}"})

        avg_train_nll = running_nll / len(train_loader)
        avg_train_cov = running_cov / len(train_loader)
        avg_train_total = avg_train_nll + avg_train_cov
        logger.info(
            f"Epoch {epoch} Train Loss: Total={avg_train_total:.4f} (NLL={avg_train_nll:.4f}, Cov={avg_train_cov:.4f})")
        tb_writer.add_scalar("Train/TotalLoss", avg_train_total, epoch)

        # éªŒè¯
        model.eval()
        val_total_loss = 0.0
        with torch.no_grad():
            val_pbar = tqdm(val_loader, desc=f"Val Epoch {epoch}/{args.num_epochs}")
            for batch in val_pbar:
                src = batch['src'].to(device)
                tgt = batch['tgt'].to(device)
                src_oov_map = batch['src_oov_map'].to(device)

                outputs, _, _, coverage_loss = model(
                    src,
                    tgt,
                    src_oov_map=src_oov_map,
                    teacher_forcing_ratio=1.0
                )

                nll_loss = calculate_nll_loss(outputs, tgt[:, 1:], pad_idx)
                val_total_loss += (nll_loss + coverage_loss).item()

        avg_val_loss = val_total_loss / len(val_loader)
        logger.info(f"Epoch {epoch} Val Loss: {avg_val_loss:.4f}")
        tb_writer.add_scalar("Val/TotalLoss", avg_val_loss, epoch)

        # [MODIFIED] ç§»é™¤åŸºäºæŒ‡æ ‡çš„è°ƒåº¦å™¨æ­¥è¿› (Warmup/Cosine æ˜¯åŸºäº step çš„ï¼Œå·²åœ¨è®­ç»ƒå¾ªç¯ä¸­å®Œæˆ)
        # scheduler.step(avg_val_loss)

        # ROUGE æŒ‡æ ‡è®¡ç®—
        if HAS_ROUGE and epoch % 2 == 0:
            logger.info("å¼€å§‹è®¡ç®—éªŒè¯é›† ROUGE æŒ‡æ ‡...")
            generated, references = generate_val_summaries(
                model=model,
                val_loader=val_loader,
                vocab=vocab,
                device=device,
                max_tgt_len=args.max_tgt_len
            )
            # è®¡ç®— ROUGE åˆ†æ•°
            rouge_scores = compute_rouge(generated, references)
            rouge1 = rouge_scores.get('rouge1_f', 0.0) * 100
            rouge2 = rouge_scores.get('rouge2_f', 0.0) * 100
            val_rouge_l = rouge_scores.get('rougeL_f', 0.0) * 100
            logger.info(
                "Epoch %d Val ROUGE-1: %.2f | ROUGE-2: %.2f | ROUGE-L: %.2f",
                epoch,
                rouge1,
                rouge2,
                val_rouge_l
            )
            tb_writer.add_scalar("Val/ROUGE-L", val_rouge_l, epoch)

            # ä¿å­˜åŸºäº ROUGE çš„æœ€ä½³æ¨¡å‹
            if val_rouge_l > best_rouge_l:
                best_rouge_l = val_rouge_l
                torch.save(model.state_dict(), Path(args.save_dir) / "best_rouge_model.pt")
                logger.info(f"âœ¨  æ–°æœ€ä½³ ROUGE æ¨¡å‹ä¿å­˜: {args.save_dir}/best_rouge_model.pt")
        elif not HAS_ROUGE:
            logger.warning("è·³è¿‡ ROUGE è®¡ç®—ï¼Œå› ä¸º 'rouge' åº“æœªå¯¼å…¥æˆ– utils/metrics ç¼ºå¤±ã€‚")

        # ä¿å­˜åŸºäºæŸå¤±çš„æœ€ä½³æ¨¡å‹
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            torch.save(model.state_dict(), Path(args.save_dir) / "best_model.pt")
            logger.info(f"âœ¨  æ–°æœ€ä½³æŸå¤±æ¨¡å‹ä¿å­˜: {args.save_dir}/best_model.pt")

        # å®šæœŸä¿å­˜ checkpoint
        if epoch % args.save_every == 0:
            ckpt_path = Path(args.save_dir) / f"checkpoint_epoch_{epoch}.pt"
            torch.save({
                "epoch": epoch,
                "model_state_dict": model.state_dict(),
                "optimizer_state_dict": optimizer.state_dict(),
                # [MODIFIED] ä¿å­˜æœ€ä½³æŒ‡æ ‡ï¼Œæ–¹ä¾¿æ¢å¤
                "best_val_loss": best_val_loss,
                "best_rouge_l": best_rouge_l,
                # "scheduler_state_dict": scheduler.state_dict(), # LambdaLR å¯é€‰ï¼Œä¸ºç®€åŒ–ä¸ä¿å­˜
                "config": {
                    "model": model_config,
                    "train": train_config,
                    "data": data_config
                }
            }, ckpt_path)
            logger.info(f"ğŸ’¾  å®šæœŸä¿å­˜æ¨¡å‹ checkpoint: {ckpt_path}")

    tb_writer.close()
    logger.info("âœ…  æ­£å¼è®­ç»ƒå®Œæˆï¼")
    logger.info(f"æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")
    if HAS_ROUGE:
        logger.info(f"æœ€ä½³éªŒè¯ ROUGE-L: {best_rouge_l:.2f}")


if __name__ == "__main__":
    # ç¤ºä¾‹è¿è¡Œå‘½ä»¤: 
    # ä»å¤´å¼€å§‹: python train_pgct_layer.py --config ../configs/pgct_layer.yaml
    # æ¢å¤è®­ç»ƒ: python train_pgct_layer.py --resume_ckpt_path ../checkpoints_pgct_layer/checkpoint_epoch_4.pt
    main()