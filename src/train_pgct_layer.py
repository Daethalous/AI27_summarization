"""
Transformer+Pointer-Generator+Coverage (PGCT) æ¨¡å‹æ­£å¼è®­ç»ƒè„šæœ¬
æ”¯æŒå®šæœŸä¿å­˜ checkpointï¼Œæ–°å¢é…ç½®æ–‡ä»¶å‚æ•°åŠ è½½ï¼ˆä¼˜å…ˆçº§ï¼šå‘½ä»¤è¡Œ>é…ç½®æ–‡ä»¶>é»˜è®¤å€¼ï¼‰
"""
from __future__ import annotations
import sys
from pathlib import Path
import logging
from typing import Optional, List
import argparse
import yaml  # æ–°å¢ï¼šå¯¼å…¥yamlåº“ç”¨äºè¯»å–é…ç½®æ–‡ä»¶

import torch
from torch.utils.data import DataLoader, Subset
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm
import torch.optim as optim
# -------------------------------------------------------------------------
# å¼•å…¥å­¦ä¹ ç‡è°ƒåº¦å™¨
from torch.optim.lr_scheduler import ReduceLROnPlateau 
# -------------------------------------------------------------------------

sys.path.insert(0, str(Path(__file__).parent.parent))

from datamodules.cnndm import prepare_datasets, get_dataloader
from utils.vocab import Vocab
# -------------------------------------------------------------------------
# å…³é”®æ”¹åŠ¨ 1: å¼•å…¥ PGCT_layer_Model å’Œè§£ç å‡½æ•°
from models.pgct_layer.pgct_layer_model import PGCT_layer_Model 
from models.pgct_layer.pgct_decoding import pgct_greedy_decode  # ç”¨äºç”ŸæˆéªŒè¯é›†æ‘˜è¦
# -------------------------------------------------------------------------

try:
    from utils.metrics import compute_rouge
    HAS_ROUGE = True
except ImportError:
    # å¦‚æœ utils.metrics ä¸å­˜åœ¨æˆ– ROUGE åº“ç¼ºå¤±ï¼Œåˆ™è®¾ç½® HAS_ROUGE ä¸º False
    HAS_ROUGE = False


def setup_logger():
    logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
    return logging.getLogger(__name__)


def calculate_nll_loss(predictions: torch.Tensor, targets: torch.Tensor, pad_idx: int) -> torch.Tensor:
    """
    è®¡ç®—è´Ÿå¯¹æ•°ä¼¼ç„¶æŸå¤± (NLL)ã€‚
    æ­¤å‡½æ•°ç°åœ¨å¿…é¡»èƒ½å¤Ÿå¤„ç† targets ä¸­çš„æ‰©å±•è¯è¡¨ç´¢å¼• (OOV è¯)ã€‚
    """
    # predictions: [B, T_out, V_ext] (V_ext = vocab_size + max_oov_size)
    # targets: [B, T_out] (T_out = T - 1, å› ä¸ºç›®æ ‡åºåˆ—ç§»ä½äº†ï¼Œtargets åŒ…å«æ‰©å±•ç´¢å¼•)
    B, T, V = predictions.shape
    preds_flat = predictions.reshape(-1, V)
    targs_flat = targets.reshape(-1)
    
    # ä½¿ç”¨ log(P) ä»¥ç¡®ä¿æ•°å€¼ç¨³å®šæ€§
    log_probs = torch.log(preds_flat + 1e-12) # é¿å… log(0)
    
    # é’ˆå¯¹ç›®æ ‡ç´¢å¼• targs_flat æ”¶é›†å¯¹åº”çš„ log æ¦‚ç‡
    # targs_flat çš„å€¼å¯ä»¥å¤§äº vocab_size (å¯¹åº” OOV è¯)
    picked = log_probs.gather(1, targs_flat.unsqueeze(1)).squeeze(1)
    
    # è®¡ç®—æœ‰æ•ˆè¯çš„æ©ç  (é PAD è¯)
    mask = (targs_flat != pad_idx).float()
    
    # NLL æŸå¤±: -log(P) çš„å¹³å‡å€¼
    loss = -(picked * mask).sum() / mask.sum()
    return loss


def generate_val_summaries(model, val_loader, vocab, device, max_tgt_len):
    """ç”ŸæˆéªŒè¯é›†æ‘˜è¦ï¼ˆç”¨äºè®¡ç®— ROUGEï¼‰"""
    model.eval()
    generated_summaries = []
    reference_summaries = []
    
    with torch.no_grad():
        for batch in tqdm(val_loader, desc="ç”ŸæˆéªŒè¯é›†æ‘˜è¦"):
            src = batch['src'].to(device)
            src_len = batch['src_len'].to(device)
            src_oov_map = batch['src_oov_map'].to(device)
            oov_dicts = batch['oov_dicts']  # æ¯ä¸ªæ ·æœ¬çš„ OOV è¯æ˜ å°„
            references = batch['tgt_text']  # å‚è€ƒæ‘˜è¦æ–‡æœ¬
            
            # è´ªå¿ƒè§£ç ç”Ÿæˆæ‘˜è¦
            pred_ids, _ = pgct_greedy_decode(
                model=model,
                src=src,
                src_lens=src_len,
                src_oov_map=src_oov_map,
                max_length=max_tgt_len,
                sos_idx=vocab.sos_idx,
                eos_idx=vocab.eos_idx,
                device=device
            )
            
            # å°†é¢„æµ‹ç´¢å¼•è½¬æ¢ä¸ºæ–‡æœ¬ï¼ˆå¤„ç† OOVï¼‰
            for i in range(len(pred_ids)):
                pred_tokens = []
                oov_dict = oov_dicts[i]  # å½“å‰æ ·æœ¬çš„ OOV æ˜ å°„
                for idx in pred_ids[i].tolist():
                    if idx < len(vocab):
                        token = vocab.idx2word.get(idx, vocab.UNK_TOKEN) #æŠ¥é”™Vocab' object has no attribute 'unk_token'
                    else:
                        oov_rel_idx = idx - len(vocab)
                        token = oov_dict.get(oov_rel_idx, vocab.UNK_TOKEN)
                    if token == vocab.EOS_TOKEN:
                        break  # é‡åˆ° EOS åœæ­¢
                    if token not in [vocab.PAD_TOKEN, vocab.SOS_TOKEN]:
                        pred_tokens.append(token)
                generated_summaries.append(' '.join(pred_tokens))
                reference_summaries.append(references[i])
    
    return generated_summaries, reference_summaries


def main():
    parser = argparse.ArgumentParser()
    # æ–°å¢ï¼šæ·»åŠ  --config å‚æ•°ï¼Œç”¨äºæŒ‡å®šYAMLé…ç½®æ–‡ä»¶è·¯å¾„
    parser.add_argument("--config", type=str, help="YAMLé…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆä¾‹å¦‚ ../configs/pgct.yamlï¼‰")
    
    # åŸæœ‰å‚æ•°ä¿ç•™ï¼Œé»˜è®¤å€¼å°†ä½œä¸ºæœ€ä½ä¼˜å…ˆçº§
    parser.add_argument("--data_dir", type=str, default="../data/raw")
    parser.add_argument("--save_dir", type=str, default="../checkpoints_pgct_layer")
    parser.add_argument("--num_epochs", type=int, default=10)
    parser.add_argument("--batch_size", type=int, default=8)
    parser.add_argument("--embed_size", type=int, default=512)
    parser.add_argument("--hidden_size", type=int, default=512)
    parser.add_argument("--num_encoder_layers", type=int, default=3)
    parser.add_argument("--num_decoder_layers", type=int, default=3)
    parser.add_argument("--nhead", type=int, default=8)
    parser.add_argument("--dropout", type=float, default=0.1)
    parser.add_argument("--cov_loss_weight", type=float, default=1.0)
    parser.add_argument("--max_src_len", type=int, default=400)
    parser.add_argument("--max_tgt_len", type=int, default=100)
    parser.add_argument("--teacher_forcing_ratio", type=float, default=0.5)
    parser.add_argument("--learning_rate", type=float, default=1e-4)
    parser.add_argument("--grad_clip", type=float, default=5.0)
    parser.add_argument("--save_every", type=int, default=2, help="éš”å¤šå°‘ä¸ª epoch ä¿å­˜ä¸€æ¬¡ checkpoint")
    parser.add_argument("--num_samples", type=int, default=None, help="é™åˆ¶è®­ç»ƒé›†ä½¿ç”¨çš„æ ·æœ¬æ•°é‡ (Noneè¡¨ç¤ºä½¿ç”¨å…¨éƒ¨)")
    # -------------------------------------------------------------------------
    args = parser.parse_args()

    logger = setup_logger()
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")

    # -------------------------------------------------------------------------
    # æ–°å¢ï¼šåŠ è½½YAMLé…ç½®æ–‡ä»¶ï¼ˆå¦‚æœæŒ‡å®šï¼‰
    config = {}
    if args.config:
        config_path = Path(args.config)
        if not config_path.exists():
            raise FileNotFoundError(f"æŒ‡å®šçš„é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
        with open(config_path, "r", encoding="utf-8") as f:
            config = yaml.safe_load(f)
        logger.info(f"âœ… æˆåŠŸåŠ è½½é…ç½®æ–‡ä»¶: {args.config}")
    # -------------------------------------------------------------------------

    # -------------------------------------------------------------------------
    # æ–°å¢ï¼šå‚æ•°ä¼˜å…ˆçº§å¤„ç†ï¼ˆå‘½ä»¤è¡Œ > é…ç½®æ–‡ä»¶ > è„šæœ¬é»˜è®¤å€¼ï¼‰
    # æ•°æ®ç›¸å…³å‚æ•°ï¼ˆå¯¹åº”é…ç½®æ–‡ä»¶dataå­—æ®µï¼‰
    data_config = config.get("data", {})
    args.data_dir = args.data_dir or data_config.get("data_dir", "../data/raw")
    args.max_src_len = args.max_src_len or data_config.get("max_src_len", 400)
    args.max_tgt_len = args.max_tgt_len or data_config.get("max_tgt_len", 100)
    max_vocab_size = data_config.get("max_vocab_size", 50000)  # è¯è¡¨å‚æ•°å•ç‹¬æå–
    min_freq = data_config.get("min_freq", 5)
    
    # æ¨¡å‹ç›¸å…³å‚æ•°ï¼ˆå¯¹åº”é…ç½®æ–‡ä»¶modelå­—æ®µï¼‰
    model_config = config.get("model", {})
    args.embed_size = args.embed_size or model_config.get("embed_size", 512)
    args.hidden_size = args.hidden_size or model_config.get("hidden_size", 512)  # ä¼˜å…ˆè¯»å–é…ç½®æ–‡ä»¶çš„hidden_size
    args.num_encoder_layers = args.num_encoder_layers or model_config.get("num_encoder_layers", 3)
    args.num_decoder_layers = args.num_decoder_layers or model_config.get("num_decoder_layers", 3)
    args.nhead = args.nhead or model_config.get("nhead", 8)
    args.dropout = args.dropout or model_config.get("dropout", 0.1)
    args.cov_loss_weight = args.cov_loss_weight or model_config.get("cov_loss_weight", 1.0)
    use_layer_attention = model_config.get("use_layer_attention", True)
    
    # è®­ç»ƒç›¸å…³å‚æ•°ï¼ˆå¯¹åº”é…ç½®æ–‡ä»¶trainå­—æ®µï¼‰
    train_config = config.get("train", {})
    args.save_dir = args.save_dir or train_config.get("save_dir", "../checkpoints_pgct")
    args.num_epochs = args.num_epochs or train_config.get("num_epochs", 10)
    args.batch_size = args.batch_size or train_config.get("batch_size", 8)
    args.learning_rate = args.learning_rate or train_config.get("learning_rate", 1e-4)
    args.teacher_forcing_ratio = args.teacher_forcing_ratio or train_config.get("teacher_forcing_ratio", 0.5)
    args.grad_clip = args.grad_clip or train_config.get("grad_clip", 5.0)
    args.save_every = args.save_every or train_config.get("save_every", 2)
    args.num_samples = args.num_samples or train_config.get("num_samples", None)
    
    # æ‰“å°æœ€ç»ˆç”Ÿæ•ˆçš„æ ¸å¿ƒå‚æ•°ï¼ˆæ–¹ä¾¿éªŒè¯ä¼˜å…ˆçº§ï¼‰
    logger.info(f"ğŸ”§ æœ€ç»ˆç”Ÿæ•ˆçš„æ ¸å¿ƒå‚æ•°:")
    logger.info(f"  - æ¨¡å‹å‚æ•°: hidden_size={args.hidden_size}, embed_size={args.embed_size}, nhead={args.nhead}")
    logger.info(f"  - è®­ç»ƒå‚æ•°: batch_size={args.batch_size}, num_epochs={args.num_epochs}, lr={args.learning_rate}")
    logger.info(f"  - æ•°æ®å‚æ•°: max_src_len={args.max_src_len}, max_tgt_len={args.max_tgt_len}, max_vocab_size={max_vocab_size}")
    # -------------------------------------------------------------------------

    # æ•°æ®é¢„å¤„ç†
    data_dir = Path(args.data_dir)
    processed_dir = data_dir.parent / "processed"
    # ä¼˜å…ˆä»é…ç½®æ–‡ä»¶è¯»å–è¯è¡¨è·¯å¾„ï¼ˆå¦‚æœæœ‰ï¼‰
    vocab_path = data_config.get("vocab_path", processed_dir / "vocab.json")
    vocab_path = Path(vocab_path)
    
    # å§‹ç»ˆè¿è¡Œ prepare_datasetsï¼Œä»¥ç¡®ä¿ PG å…¼å®¹çš„åŸå§‹ tokens è¢«ä¿å­˜
    logger.warning(f"ç¡®ä¿ {processed_dir} ä¸­çš„ç¼“å­˜æ–‡ä»¶åŒ…å« PG æ‰€éœ€çš„åŸå§‹ tokensï¼Œè‹¥æ²¡æœ‰ï¼Œå°†é‡æ–°ç”Ÿæˆæ•°æ®...")
    prepare_datasets(
        str(data_dir), 
        str(processed_dir), 
        str(vocab_path),
        max_vocab_size=max_vocab_size,  # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„è¯è¡¨å¤§å°
        min_freq=min_freq,              # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„æœ€å°è¯é¢‘
        max_src_len=args.max_src_len, 
        max_tgt_len=args.max_tgt_len
    )

    vocab = Vocab.load(str(vocab_path))
    pad_idx = vocab.pad_idx
    sos_idx = vocab.sos_idx
    eos_idx = vocab.eos_idx
    logger.info(f"è¯è¡¨å·²åŠ è½½: {len(vocab)} ä¸ªè¯")

    # -------------------------------------------------------------------------
    # ä¸º PG æ¨¡å‹åŠ è½½ DataLoader æ—¶ï¼Œå¿…é¡»ä¼ é€’ vocab å¹¶è®¾ç½® include_oov=True
    full_train_loader = get_dataloader(
        str(processed_dir), 
        batch_size=args.batch_size, 
        split="train", 
        shuffle=True, 
        vocab=vocab,
        include_oov=True # å¯ç”¨ PG æœºåˆ¶
    )
    
    if args.num_samples is not None and args.num_samples < len(full_train_loader.dataset):
        # åˆ›å»ºä¸€ä¸ªæ•°æ®é›†å­é›†
        indices = list(range(args.num_samples))
        subset_dataset = Subset(full_train_loader.dataset, indices)
        # ç›´æ¥ä½¿ç”¨ full_train_loader.collate_fnï¼ˆPGCollateFn å®ä¾‹ï¼‰
        train_loader = DataLoader(
            subset_dataset, 
            batch_size=args.batch_size, 
            shuffle=True, 
            collate_fn=full_train_loader.collate_fn,
            num_workers=full_train_loader.num_workers
        )
        logger.info(f"ğŸš§ é™åˆ¶è®­ç»ƒé›†å¤§å°ä¸º {args.num_samples} ä¸ªæ ·æœ¬ã€‚")
    else:
        train_loader = full_train_loader
        
    val_loader = get_dataloader(
        str(processed_dir), 
        batch_size=args.batch_size, 
        split="val", 
        shuffle=False,
        vocab=vocab,
        include_oov=True # éªŒè¯é›†åŒæ ·éœ€è¦ PG æœºåˆ¶
    )
    # -------------------------------------------------------------------------


    # æ¨¡å‹ï¼ˆå‚æ•°å·²é€šè¿‡ä¼˜å…ˆçº§å¤„ç†ï¼Œç›´æ¥ä½¿ç”¨ args ä¸­çš„å€¼ï¼‰
    model = PGCT_layer_Model(
        vocab_size=len(vocab),
        embed_size=args.embed_size,
        hidden_size=args.hidden_size,  # æ­¤å¤„å·²ä¼˜å…ˆä½¿ç”¨å‘½ä»¤è¡Œ/é…ç½®æ–‡ä»¶çš„å‚æ•°
        num_encoder_layers=args.num_encoder_layers,
        num_decoder_layers=args.num_decoder_layers,
        nhead=args.nhead,
        dropout=args.dropout,
        pad_idx=pad_idx,
        cov_loss_weight=args.cov_loss_weight,
        max_src_len=args.max_src_len,
        max_tgt_len=args.max_tgt_len,
        use_layer_attention=use_layer_attention
    ).to(device)
    logger.info("PGCT_layer_Model åˆå§‹åŒ–å®Œæˆ")
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    Path(args.save_dir).mkdir(parents=True, exist_ok=True)
    
    optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)
    
    tb_writer = SummaryWriter(log_dir=Path(args.save_dir)/"runs")

    best_val_loss = float("inf")
    best_rouge_l = -float("inf")  # ç”¨äºè·Ÿè¸ªæœ€ä½³ ROUGE åˆ†æ•°

    for epoch in range(1, args.num_epochs+1):
        model.train()
        running_nll = 0.0
        running_cov = 0.0
        pbar = tqdm(train_loader, desc=f"Train Epoch {epoch}/{args.num_epochs}")
        
        for batch in pbar:
            src = batch['src'].to(device)
            # tgt ç°åœ¨æ˜¯æ‰©å±•è¯è¡¨ç´¢å¼• (tgt_ext)
            tgt = batch['tgt'].to(device) 
            
            # src_oov_map ç”¨äº Pointer-Generator æœºåˆ¶ (æºæ–‡æœ¬çš„æ‰©å±•ç´¢å¼•)
            src_oov_map = batch['src_oov_map'].to(device)

            optimizer.zero_grad()
            
            # PGCT_layer_Model.forward è®¡ç®—è¾“å‡ºå’ŒæŸå¤±
            outputs, _, _, coverage_loss = model(
                src, 
                tgt, 
                src_oov_map=src_oov_map, 
                teacher_forcing_ratio=args.teacher_forcing_ratio
            ) # outputs: [B, T_out, V_ext]
            
            # ç›®æ ‡åºåˆ—éœ€è¦ç§»ä½ (ç§»é™¤ SOS ä»¤ç‰Œ)
            nll_loss = calculate_nll_loss(outputs, tgt[:, 1:], pad_idx)
            
            # æ€»æŸå¤± = NLL Loss + Coverage Loss
            total_loss = nll_loss + coverage_loss
            
            total_loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=args.grad_clip)
            optimizer.step()

            running_nll += nll_loss.item()
            running_cov += coverage_loss.item()
            pbar.set_postfix({"NLL": f"{nll_loss.item():.4f}", "Cov": f"{coverage_loss.item():.4f}", "LR": optimizer.param_groups[0]['lr']})

        avg_train_nll = running_nll / len(train_loader)
        avg_train_cov = running_cov / len(train_loader)
        avg_train_total = avg_train_nll + avg_train_cov
        logger.info(f"Epoch {epoch} Train Loss: Total={avg_train_total:.4f} (NLL={avg_train_nll:.4f}, Cov={avg_train_cov:.4f})")
        tb_writer.add_scalar("Train/TotalLoss", avg_train_total, epoch)

        # éªŒè¯
        model.eval()
        val_total_loss = 0.0
        with torch.no_grad():
            val_pbar = tqdm(val_loader, desc=f"Val Epoch {epoch}/{args.num_epochs}")
            for batch in val_pbar:
                src = batch['src'].to(device)
                tgt = batch['tgt'].to(device)
                src_oov_map = batch['src_oov_map'].to(device)

                # éªŒè¯æ—¶ teacher_forcing_ratio è®¾ä¸º 1.0
                outputs, _, _, coverage_loss = model(
                    src, 
                    tgt, 
                    src_oov_map=src_oov_map, 
                    teacher_forcing_ratio=1.0
                )
                
                nll_loss = calculate_nll_loss(outputs, tgt[:, 1:], pad_idx)
                val_total_loss += (nll_loss + coverage_loss).item()

        avg_val_loss = val_total_loss / len(val_loader)
        logger.info(f"Epoch {epoch} Val Loss: {avg_val_loss:.4f}")
        tb_writer.add_scalar("Val/TotalLoss", avg_val_loss, epoch)
        
        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step(avg_val_loss)
        
        # -------------------------------------------------------------------------
        # ROUGE æŒ‡æ ‡è®¡ç®—ï¼ˆå®Œæ•´å®ç°ï¼‰
        if HAS_ROUGE and epoch % 2 == 0:  # æ¯2ä¸ªepochè®¡ç®—ä¸€æ¬¡ROUGEï¼ˆèŠ‚çœæ—¶é—´ï¼‰
            logger.info("å¼€å§‹è®¡ç®—éªŒè¯é›† ROUGE æŒ‡æ ‡...")
            generated, references = generate_val_summaries(
                model=model,
                val_loader=val_loader,
                vocab=vocab,
                device=device,
                max_tgt_len=args.max_tgt_len
            )
           # è®¡ç®— ROUGE åˆ†æ•°
            rouge_scores = compute_rouge(generated, references)
            rouge1 = rouge_scores.get('rouge1_f', 0.0) * 100
            rouge2 = rouge_scores.get('rouge2_f', 0.0) * 100
            val_rouge_l = rouge_scores.get('rougeL_f', 0.0) * 100
            logger.info(
                "Epoch %d Val ROUGE-1: %.2f | ROUGE-2: %.2f | ROUGE-L: %.2f",
                epoch,
                rouge1,
                rouge2,
                val_rouge_l
            )
            tb_writer.add_scalar("Val/ROUGE-L", val_rouge_l, epoch)
            # å¯é€‰ï¼šä¿å­˜åŸºäº ROUGE çš„æœ€ä½³æ¨¡å‹
            if val_rouge_l > best_rouge_l:
                best_rouge_l = val_rouge_l
                torch.save(model.state_dict(), Path(args.save_dir)/"best_rouge_model.pt")
                logger.info(f"âœ¨ æ–°æœ€ä½³ ROUGE æ¨¡å‹ä¿å­˜: {args.save_dir}/best_rouge_model.pt")
        elif not HAS_ROUGE:
            logger.warning("è·³è¿‡ ROUGE è®¡ç®—ï¼Œå› ä¸º 'rouge' åº“æœªå¯¼å…¥æˆ– utils/metrics ç¼ºå¤±ã€‚")
        # -------------------------------------------------------------------------


        # ä¿å­˜åŸºäºæŸå¤±çš„æœ€ä½³æ¨¡å‹
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            torch.save(model.state_dict(), Path(args.save_dir)/"best_model.pt")
            logger.info(f"âœ¨ æ–°æœ€ä½³æŸå¤±æ¨¡å‹ä¿å­˜: {args.save_dir}/best_model.pt")

        # å®šæœŸä¿å­˜ checkpoint
        if epoch % args.save_every == 0:
            ckpt_path = Path(args.save_dir)/f"checkpoint_epoch_{epoch}.pt"
            torch.save({
                "epoch": epoch,
                "model_state_dict": model.state_dict(),
                "optimizer_state_dict": optimizer.state_dict(),
                "avg_val_loss": avg_val_loss,
                "config": {  # ä¿å­˜å½“å‰è®­ç»ƒå‚æ•°ï¼Œæ–¹ä¾¿æ¨ç†æ—¶å¯¹é½
                    "model": model_config,
                    "train": train_config,
                    "data": data_config
                }
            }, ckpt_path)
            logger.info(f"ğŸ’¾ å®šæœŸä¿å­˜æ¨¡å‹ checkpoint: {ckpt_path}")

    tb_writer.close()
    logger.info("âœ… æ­£å¼è®­ç»ƒå®Œæˆï¼")
    logger.info(f"æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")
    if HAS_ROUGE:
        logger.info(f"æœ€ä½³éªŒè¯ ROUGE-L: {best_rouge_l:.2f}")


if __name__ == "__main__":
    # ç¤ºä¾‹è¿è¡Œå‘½ä»¤ (åœ¨ src ç›®å½•ä¸‹): 
    # python train_pgct_layer.py --config ../configs/pgct_layer.yaml --batch_size 10
    main()
