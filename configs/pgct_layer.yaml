# 数据配置（适配 PGCT 模型的 OOV 处理）
data:
  max_vocab_size: 50000
  min_freq: 5
  max_src_len: 400
  max_tgt_len: 100
  vocab_path: ../data/processed/vocab.json
  data_dir: ../data/raw
  processed_dir: ../data/processed

# 模型配置（PGCT 核心参数：Transformer + Pointer-Generator + Coverage + layer_attention）
model:
  type: pgct_layer
  embed_size: 512
  hidden_size: 512
  num_encoder_layers: 2
  num_decoder_layers: 2
  nhead: 4
  dropout: 0.1
  cov_loss_weight: 1.0
  pad_idx: 0
  sos_idx: 2
  eos_idx: 3
  use_layer_attention: true

# 训练配置 (已适配 AdamW 和 Warmup + Cosine Annealing)
train:
  batch_size: 8
  num_epochs: 10
  learning_rate: 1e-4
  weight_decay: 0.01
  warmup_steps: 4000
  grad_clip: 5.0
  teacher_forcing_ratio: 0.5
  save_every: 2
  num_samples: null

# 评估配置（支持 Greedy/Beam 两种解码策略）
eval:
  split: test
  batch_size: 32
  decode_strategy: greedy
  beam_size: 5
  show_examples: 3
  output_dir: ../outputs_pgct_layer
  output_file: test_summaries.json
  num_workers: 0
